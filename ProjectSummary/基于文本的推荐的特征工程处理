一.首先，总结下大概流程和常用的特征提取方法。别忘了再写到笔记本里。
1.流程:
(1)将词分成各个block,进行特征工程处理。
(2)转换成计算机可理解的数字。
(3)最后进行回归/分类的机器学习操作。

2.特征工程过程
(1)分词(Tokenize)
英文:
根据空格用: tokens = nltk.word_tokenize(sentence) =>词列表
直接用split处理，将字符串.split()成列表，用" ".join([])格式处理。

中文:
机器学习：HMM,CRF
启发式:Heuristic ->利用词典对照表将这句话拆成各个单词放在列表里，类似贪婪算法。
jieba:默认精准
(1) 全模式: jieba.cut("我/来到/北京/清华/清华大学/华大/大学",cut_all = True) 所有可能性都分出。
(2) 精确模式: jieba.cut("我/来到/北京/清华大学",cut_all = False) 
(3) 新闻识别: 他来到了网易杭研大厦。 杭研不是单词，但能被这个Viterbi算法识别。
(4) 搜索引擎模式: 类似全模式，但内容更多，把搜索组合都排列。 jieba.cut_for_search

分词存在问题:
(1)
有很多乱七八糟的语法不合逻辑语言很多。
比如骂人话，表情符号 @ 等。
解决:
用正则表达式处理。
常用的中文正则表达式处理:
“^a”：匹配所有a开始的字符串 
“a$”：匹配所有a结尾的字符串 
前面一般都加\
1.小写字母就是表示匹配该类型的，大写字母就是小写字母的反向匹配。
比如: \w :匹配大小写字母的 \W:匹配不是大小写字母的
  \d: 匹配数字的  \D:匹配非数字的
  \s:匹配特殊字符的 \S:匹配非特殊字符的

2: ?  0或1
   * 任意
   +  1或多
   
3. d{2,4} 匹配2到4个数字的，也就是2，3，4个数字都可以。
4.[abc] 我们自己设定字符abc进行匹配。 或者[a-zA-z]这样匹配a-z的大小写英文字符
5.  .  点 除了换行符所有都匹配
     .* 就是所有
6.$ 以什么结束
   ^以什么开始
   以W开始以e结束的 ^We$

7.(at) | (ve) 所有的at 和所有的ve都匹配出来。中间|是或。

8.分组: (...).*\1 前面三个点表示前面三个字母，然后.*表示所有匹配，后面\1表示前面的三个字母会在后面中再重复出现一次，记住不一定是最后三个。

常用的英文正则表达式处理:
  s = re.sub(r"(\w)\.([A-Z])", r"\1 \2", s)

        s = s.lower()
        s = s.replace("  ", " ")
        s = re.sub(r"([0-9]),([0-9])", r"\1\2", s)
        s = s.replace(",", " ")
        s = s.replace("$", " ")
        s = s.replace("?", " ")
        s = s.replace("-", " ")
        s = s.replace("//", "/")
        s = s.replace("..", ".")
        s = s.replace(" / ", " ")
        s = s.replace(" \\ ", " ")
        s = s.replace(".", " . ")
        s = re.sub(r"(^\.|/)", r"", s)
        s = re.sub(r"(\.|/)$", r"", s)
        s = re.sub(r"([0-9])([a-z])", r"\1 \2", s)
        s = re.sub(r"([a-z])([0-9])", r"\1 \2", s)
        s = s.replace(" x ", " xbi ")
        s = re.sub(r"([a-z])( *)\.( *)([a-z])", r"\1 \4", s)
        s = re.sub(r"([a-z])( *)/( *)([a-z])", r"\1 \4", s)
        s = s.replace("*", " xbi ")
        s = s.replace(" by ", " xbi ")
        s = re.sub(r"([0-9])( *)\.( *)([0-9])", r"\1.\4", s)
        s = re.sub(r"([0-9]+)( *)(inches|inch|in|')\.?", r"\1in. ", s)
        s = re.sub(r"([0-9]+)( *)(foot|feet|ft|'')\.?", r"\1ft. ", s)
        s = re.sub(r"([0-9]+)( *)(pounds|pound|lbs|lb)\.?", r"\1lb. ", s)
        s = re.sub(r"([0-9]+)( *)(square|sq) ?\.?(feet|foot|ft)\.?", r"\1sq.ft. ", s)
        s = re.sub(r"([0-9]+)( *)(cubic|cu) ?\.?(feet|foot|ft)\.?", r"\1cu.ft. ", s)
        s = re.sub(r"([0-9]+)( *)(gallons|gallon|gal)\.?", r"\1gal. ", s)
        s = re.sub(r"([0-9]+)( *)(ounces|ounce|oz)\.?", r"\1oz. ", s)
        s = re.sub(r"([0-9]+)( *)(centimeters|cm)\.?", r"\1cm. ", s)
        s = re.sub(r"([0-9]+)( *)(milimeters|mm)\.?", r"\1mm. ", s)
        s = s.replace("°", " degrees ")
        s = re.sub(r"([0-9]+)( *)(degrees|degree)\.?", r"\1deg. ", s)
        s = s.replace(" v ", " volts ")
        s = re.sub(r"([0-9]+)( *)(volts|volt)\.?", r"\1volt. ", s)
        s = re.sub(r"([0-9]+)( *)(watts|watt)\.?", r"\1watt. ", s)
        s = re.sub(r"([0-9]+)( *)(amperes|ampere|amps|amp)\.?", r"\1amp. ", s)
        s = s.replace("  ", " ")
        s = s.replace(" . ", " ")
        
(2)词性变化
英文常见
walk => walking => walked 不影响词性。

解决方案:
归一化
a.Stemming：词干提取，把不影响词性的尾巴去掉。
b.Lemmatization词形归: 把各种类型的词的变形,都归于一个形式。
went归一=go
are归一be

中文可以用jieba进行词性标注。
import jieba.posseg as pseg
words = pseg.cut("我爱北京大学")
for word,flag in words:
  print('%s %s' %(word,flag))
  
英文词性标注
import nltk
sent = "I am going to Beijing tomorrow";
tokens = nltk.word_tokenize(sent)
taged_sent = nltk.pos_tag(tokens)
print(taged_sent)
  
(2)jieba常用方法:(分词不说了，上面说完了)
a.自定义词典
jieba.load_userdict(filename) # filename为自定义词典的路径
在这里定义词典，直接用jieba加载即可
格式每行是词的值，词频 ，词性。 必须这个格式

b.关键词提取(TF-IDF,Word2vec都有这个，也就是NLP的特征提取)
关键词抽取并且进行词向量化之后，才好进行下一步的文本分析(特征提取)
import jieba.analyse
jieba.analyse.extract_tags(sentence,topK = 20,withWeight = False,allowPos())
sentence 为待提取的文本
topK为返回几个TF/IDF权重最大的关键词,默认值为20
withWeight为是否一并返回关键词权重值，默认值为False
allowPOS仅包含指定词性的词，默认值为空,即不筛选。
举例:
方法1：
import jieba.analyse
for x,w in jieba.analyse.extract_tags(s,withWeight = True);
  print('%s %s' % (x,w))

for x,w in jieba.analyse.textrank(s,withWeight = True)
  print('%s %s' % (x,w))
  
c.并行分词
jieba.enable_parallel(4) 四核。
jieba.disable_parallel() 关闭

3.项目流程总结

三.文本流程
1步: 将一个长句子，分割成几个小字符串。
2步: 将一些没意义的词去掉，比如冠词，虚词等。
3步: 将词语进行数字化形式表达。
4步: 通过机器学习/深度学习处理进行分类。

具体分析:
1.获取一个列表的文字内容
2.分词算法大部分用HMM或CRF 也有传统的启发式分词(就是一个个搜索)
3.用Tokenize也就是分词技术进行分词，通常我们用jeba分词来分解中文。
4.对于复杂的语句需要用正则表达式分词。
5.对于词性复杂的问题，有以下俩种方案:
(1)Stemming词干提取:简单说就是把不影响词性的inflection的小尾巴砍掉。
walking = walk
(2)Lemmatization词形归一: 把各种类型的词的变形，都归为⼀个形式
went 归一成 go 
are 归一成 be
但有个缺点:但went为动词时，是go的过去式，名词是人名。
6.所以需要用NLTK的 POS Tag来判断该单词的词性。
7.然后是停止词，这类词都是理解文本意思的应用场景来说，岐义太多。
8.最后生成word_list列表再进行机器学习处理。(SVM,RF,MLP,RNN,LSTM)

二.文本常用函数以及相应的特征提取方法。
jieba已经介绍完了，现在介绍其他常用函数。
https://blog.csdn.net/kumoshu/article/details/79858748
1.CountVectorizer:将文本转化为词频计数矩阵。

如果您不提供先验词典并且不使用执行某种特征选择的分析器，则功能的数量将等于通过分析数据找到的词汇量。
结果是生成一个稀疏矩阵， 等价于 scipy.sparse.csr_matrix
encoding ：字符格式
ngram_range：要提取的不同n-gram的n值范围的下边界和上边界。将使用n的所有值，使得min_n <= n <= max_n。
n-gram你可以理解为通过一个范围的文字，来判断这个词的概率是多少。
stop_words：停止词。

CountVectorizer(ngram_range=(2,2))#2-gram序列 
意思是2个词合成一个进行频率统计：[('and the', 0), ('is shining', 1), ('is sunning', 2), ('is sweet', 3)]
count_vector = CountVectorizer(ngram_range=(2,2))#2-gram序列
bag = count_vector.fit_transform(docs)
print(sorted(count_vector.vocabulary_.items(),key=lambda x:x[1]))#2-gram序列生成的词汇表
bag.toarray()


下面三个函数，常用函数都跟这个一样。
2.HashingVectorizer
它将文本文档的集合转换为scipy.sparse矩阵，其中包含令牌出现计数（或二进制出现信息），
如果norm ='l1'，则可能标准化为词频，如果norm ='l2'则投影在词距离
此文本向量化程序实现使用散列技巧来查找字符串名称以进行整数索引映射。

这种策略有几个优点：
它是非常低的内存可扩展到大型数据集，因为不需要在内存中存储词汇词典
它除了构造函数参数之外没有任何状态，所以它很快发生酸洗和解毒
它可用于流式（部分拟合）或并行管道，因为在拟合期间没有计算状态。
还有一些缺点（与使用具有内存词汇表的CountVectorizer相比）：

没有办法计算逆变换（从特征索引到字符串特征名称），当试图内省哪些特征对模型最重要时，这可能是一个问题。
可能存在冲突：不同的令牌可以映射到相同的特征索引。但是在实践中，如果n_features足够大（例如，文本分类问题为2 ** 18），这很少成为问题。
没有IDF加权，因为这会使变压器有状态。

3.TfidfTransformer:一个词在文档出现的太频繁,就需要用TF-IDF来表示。
将计数矩阵转换为标准化的tf或tf-idf表示
但，记住，这不是矩阵表示，只是标准化到tf-idf.一般用下面那个表示tf-idf

4.TfidVectorizer:
将原始文档集合转换为TF-IDF特征矩阵。
相当于CountVectorizer，后跟TfidfTransformer。
tfidf = TfidfVectorizer()
np.set_printoptions(precision=2)
tfidf.fit_transform(docs).toarray()

5.常用提取特征方法.其实大部分还是词频和TF-IDF多。尤其是多文本基本就用TF-IDF。
为什么用特征提取:
仍然有9400多个词汇。而用这近万个词汇作为特征词的话，词向量所占用的空间以及运算的时间将会是个灾难。而且，某些词汇存在的意义对于文本分类十分小，
如果其作为特征词的话，对于文本分类效果产生负面影响。因此，有必要在词向量构建前，进行特征选取，也就是特征词的选取。主要方法有以下五种。

(1) 基于文档频率的特征提取法 
文档频率(DF)是指出现在某个特征项的文档的频率。当特征项的文档频率大于设定的上界时或者小于设定的下界时，则会在特征项集中去除该特征项。

(2) 基于特征项频率的特征提取方法 
特征项频率(TF)是指特征项在文本几何中出现的频率。选择方法、存在问题与上述方法一致。

(3) 基于逆文档频率的特征提取法 
逆文档频率(TF-IDF)是指特征项频率与出现特征项的文档逆频率的乘积。 
TF-IDF = TF * IDF = tf * (N/n) 
其中tf为特征项频率，N为训练集文档数目，n为训练集中出现特征项w的文档数。

(4)卡方统计量 
χ2统计量表示特征项与类别的相关程度，χ2统计量值越大，则两者的相关程度越高。其计算公式如下：χ2(wi,Cd)=N∗(A∗D−C∗B)(A+C)∗(B+D)∗(A+B)∗(C+D) 
一般通过选择特征项wi与某一类别Cd的最大值作为其χ2统计量。之后，通过设定阈值或者根据卡方统计量大小排序选择的方式进行特正项的选取。

(5)互信息法 
互信息值表示特征项与类别的共现程度，互信息值越大，则两者的共现概率越大。其计算公式如下： 
I(wi,Cd)≈logA∗N(A+C)∗(A+B) 
一般通过选择特征项wi与某一类别Cd的最大值作为其互信息值。之后，通过设定阈值或者根据互信息值大小排序选择的方式进行特正向的选取。

总结：使用朴素贝叶斯分类器进行文本分类就能达到较好的分类效果，主要优化为特征选取与权重调整两个方面。本次只讨论了简要
的分类器构造过程以及特征选取方法，对于权重调整也可以用到特征选取中提到的算法，从而提高分类器的准确率。而且，之后也可以利用
深度学习中CNN卷积神经网络实现文本的分类，其性能准确性有待考证。
  
  
 四.NLTK常用方法
 https://blog.csdn.net/u011630575/article/details/80158788
 
 五.word2vec常用方法
常用方法：
import gensim
# 导入模型
model = gensim.models.Word2Vec.load("wiki.zh.text.model")

4. 模型使用
可以参照官网上的指导迅速了解model的各种功能方法。

4.0 获取词向量
        print model[u'汽车']
        type(model[u'汽车'])

# 结果
[  3.74845356e-01   1.86477005e+00   1.28353190e+00   8.04618478e-01 ... ]
numpy.ndarray

4.1 计算一个词的最近似的词，倒排序
result = model.most_similar(u'足球')
for each in result:
    print each[0] , each[1]

国际足球 0.556692957878
足球运动 0.530436098576
篮球 0.518306851387
国家足球队 0.516140639782
足球队 0.513238489628
足球联赛 0.500901579857
football 0.500162124634
体育 0.499264538288
足球比赛 0.488131582737
冰球 0.48725092411

4.2 计算两词之间的余弦相似度
word2vec一个很大的亮点：支持词语的加减运算。（实际中可能只有少数例子比较符合）

>>> model.most_similar(positive=['woman', 'king'], negative=['man'])
[('queen', 0.50882536), ...]

sim1 = model.similarity(u'勇敢', u'战斗')
sim2 = model.similarity(u'勇敢', u'胆小')
sim3 = model.similarity(u'高兴', u'开心')
sim4 = model.similarity(u'伤心', u'开心')
print sim1 
print sim2
print sim3
print sim4

0.254622852224
0.38974887559
0.423695453969
0.376244588456

4.3 计算两个集合之间的余弦似度
当出现某个词语不在这个训练集合中的时候，会报错！！！。

list1 = [u'今天', u'我', u'很', u'开心']
list2 = [u'空气',u'清新', u'善良', u'开心']
list3 = [u'国家电网', u'再次', u'宣告', u'破产', u'重新']
list_sim1 =  model.n_similarity(list1, list2)
print list_sim1
list_sim2 = model.n_similarity(list1, list3)
print list_sim2

0.541874230659
0.13056320154

4.4 选出集合中不同类的词语
list = [u'纽约', u'北京', u'上海', u'西安']
print model.doesnt_match(list)
list = [u'纽约', u'北京', u'上海', u'西瓜']
print model.doesnt_match(list)

纽约
西瓜
 
介绍：
使用Gensim训练Word2vec十分方便，训练步骤如下：

1.将语料库预处理：一行一个文档或句子，将文档或句子分词（以空格分割，英文可以不用分词，英文单词之间已经由空格分割，中文预料需要使用分词工具进行分词，常见的分词工具有StandNLP、ICTCLAS、Ansj、FudanNLP、HanLP、结巴分词等）；

2.将原始的训练语料转化成一个sentence的迭代器，每一次迭代返回的sentence是一个word（utf8格式）的列表。可以使用Gensim中word2vec.py中的LineSentence()方法实现；

3.将上面处理的结果输入Gensim内建的word2vec对象进行训练即可：

from gensim.models import Word2Vec
model = Word2Vec(LineSentence(inp), size=100, window=10, min_count=3,
            workers=multiprocessing.cpu_count(), sg=1, iter=10, negative=20)
具体的训练参数解释如下：
class Word2Vec(utils.SaveLoad):
    def __init__(
            self, sentences=None, size=100, alpha=0.025, window=5, min_count=5,
            max_vocab_size=None, sample=1e-3, seed=1, workers=3, min_alpha=0.0001,
            sg=0, hs=0, negative=5, cbow_mean=1, hashfxn=hash, iter=5, null_word=0,
            trim_rule=None, sorted_vocab=1, batch_words=MAX_WORDS_IN_BATCH):

· sentences：可以是一个list，对于大语料集，建议使用BrownCorpus,Text8Corpus或lineSentence构建。
· size：是指特征向量的维度，默认为100。

· alpha: 是初始的学习速率，在训练过程中会线性地递减到min_alpha。

· window：窗口大小，表示当前词与预测词在一个句子中的最大距离是多少。

· min_count: 可以对字典做截断. 词频少于min_count次数的单词会被丢弃掉, 默认值为5。

· max_vocab_size: 设置词向量构建期间的RAM限制，设置成None则没有限制。

· sample: 高频词汇的随机降采样的配置阈值，默认为1e-3，范围是(0,1e-5)。

· seed：用于随机数发生器。与初始化词向量有关。

· workers：用于控制训练的并行数。

· min_alpha：学习率的最小值。

· sg： 用于设置训练算法，默认为0，对应CBOW算法；sg=1则采用skip-gram算法。

· hs: 如果为1则会采用hierarchica·softmax技巧。如果设置为0（默认），则使用negative sampling。

· negative: 如果>0,则会采用negativesampling，用于设置多少个noise words（一般是5-20）。

· cbow_mean: 如果为0，则采用上下文词向量的和，如果为1（default）则采用均值，只有使用CBOW的时候才起作用。

· hashfxn： hash函数来初始化权重，默认使用python的hash函数。

· iter： 迭代次数，默认为5。

· trim_rule： 用于设置词汇表的整理规则，指定那些单词要留下，哪些要被删除。可以设置为None（min_count会被使用）。

· sorted_vocab： 如果为1（默认），则在分配word index 的时候会先对单词基于频率降序排序。

· batch_words：每一批的传递给线程的单词的数量，默认为10000。


一些参数的选择与对比：

1.skip-gram （训练速度慢，对罕见字有效），CBOW（训练速度快）。一般选择Skip-gram模型；

2.训练方法：Hierarchical Softmax（对罕见字有利），Negative Sampling（对常见字和低维向量有利）；

3.欠采样频繁词可以提高结果的准确性和速度（1e-3~1e-5）

4.Window大小：Skip-gram通常选择10左右，CBOW通常选择5左右。


  
  
 
 六.本次项目，常用的NLP特征工程和方法处理。
 1.构造特征
 (1)构造词长度的特征:df_all['len_of_query'] = df_all['search_term'].map(lambda x: len(x.split())).astype(np.int64)
 (2)构造不同文本之间相同词并取出来的特征：df_all['search_term'] = df_all['product_info'].map(lambda x: seg_words(x.split('\t')[0], x.split('\t')[1]))
 (3)构造文本之间是否有相同词出现，返回个数的特征：df_all['query_in_title'] = df_all['product_info'].map(lambda x: str_whole_word(x.split('\t')[0], x.split('\t')[1], 0))
 (4)构造一个文本词循环在另一个文本词中出现的个数，如果没有，用edit_distance相似度量来计算，如果是中文，换成余弦相似度来计算。
 其他看项目介绍即可。
 
 2.数据预处理常用方法
 (1)统计我们样本某个需求的数量,比如品牌:
 self.brands = X['brands'].value_counts().index[:self.num_brands]
 
 (2)标注nan值，这个一般跟数据型数据不太一样，中文或者英文均可自己设定，我们习惯性设计unkown吧。
 X['category_name'] = X['category_name'].fillna('unkown')
 或者fillna('')均可，看自己需求。
 
 (3)处理敏感词
 不同业务都有敏感词，如果公司业务有词典最好，没有，需要自己根据业务来设计。
 比如:我们这次项目是商品: 1-9，需要转化为str, X10,X13这种会最终影响价格。
 还有，如果是乔丹品牌，分中国乔丹，美国乔丹，我们做推荐价格业务时，为了让结果更准确，需要在乔丹
 前面做个判断，也就是乔丹这个词必须要做成字典，然后用jieba.load它。然后根据乔丹前面的词，来衡量结果。
 这样的案例有很多，这块处理，更是考验我们的业务知识。
 
 (4)特征构造，新型方法。
 在上文基础上，当业务复杂到包含时间范围时，我们需要构造关于时间的年份。
 因为做推荐时，商品的价格也是随时间波动而波动。
 然后还有男人，女人，小孩等等，都可以依次构造特征，比如
 XXX.apply(lambda x:x[0] if x[0] != 'Women' else '/'.join(x[:2]))
 去掉商品描述里含有女人，并单独拿出这一特征。
 
 (5)处理可能出现的干扰词。
 英文：for like fits
 中文: 像 大概 这样
 
 (6)优化器，矩阵压缩。这次本次项目的核心点。
 这个不好理解，多看几遍。
一. scipy.sparse.csr_matrix:
indptr = np.array([0, 2, 3, 6])
indices = np.array([0, 2, 2, 0, 1, 2])
data = np.array([1, 2, 3, 4, 5, 6])
csr_matrix((data, indices, indptr), shape=(3, 3)).toarray()
array([[1, 0, 2],
       [0, 0, 3],
       [4, 5, 6]])
# 按row行来压缩
# 对于第i行，非0数据列是indices[indptr[i]:indptr[i+1]] 数据是data[indptr[i]:indptr[i+1]]
# 在本例中
# 第0行，有非0的数据列是indices[indptr[0]:indptr[1]] = indices[0:2] = [0,2]
# 数据是data[indptr[0]:indptr[1]] = data[0:2] = [1,2],所以在第0行第0列是1，第2列是2
# 第1行，有非0的数据列是indices[indptr[1]:indptr[2]] = indices[2:3] = [2]
# 数据是data[indptr[1]:indptr[2] = data[2:3] = [3],所以在第1行第2列是3
# 第2行，有非0的数据列是indices[indptr[2]:indptr[3]] = indices[3:6] = [0,1,2]
# 数据是data[indptr[2]:indptr[3]] = data[3:6] = [4,5,6],所以在第2行第0列是4，第1列是5,第2列是6
 
  
 二.scipy.sparse.csc_matrix
indptr = np.array([0, 2, 3, 6])
indices = np.array([0, 2, 2, 0, 1, 2])
data = np.array([1, 2, 3, 4, 5, 6])
csc_matrix((data, indices, indptr), shape=(3, 3)).toarray()
array([[1, 0, 4],
       [0, 0, 5],
       [2, 3, 6]])
# 按col列来压缩
# 对于第i列，非0数据行是indices[indptr[i]:indptr[i+1]] 数据是data[indptr[i]:indptr[i+1]]
# 在本例中
# 第0列，有非0的数据行是indices[indptr[0]:indptr[1]] = indices[0:2] = [0,2]
# 数据是data[indptr[0]:indptr[1]] = data[0:2] = [1,2],所以在第0列第0行是1，第2行是2
# 第1行，有非0的数据行是indices[indptr[1]:indptr[2]] = indices[2:3] = [2]
# 数据是data[indptr[1]:indptr[2] = data[2:3] = [3],所以在第1列第2行是3
# 第2行，有非0的数据行是indices[indptr[2]:indptr[3]] = indices[3:6] = [0,1,2]
# 数据是data[indptr[2]:indptr[3]] = data[3:6] = [4,5,6],所以在第2列第0行是4，第1行是5,第2行是6

(7)特征提取(在用TF-IDF,TSVD前，因为我们之前构造了大量特征，我们先用SGDRegressor去查看特征性)
用SGDRegressor提取特征的原因是：我们这次用深度学习去拟合模型，而深度学习虽然最终是非线性模型，但它的本质是大量的线性通过
softmax构建成非线性模型，所以我们采用SGDRegressor。
 sgd = SGDRegressor(penalty='l1', loss='squared_loss', alpha=3.0e-11, power_t=-0.12, eta0=0.019, random_state=0,
                           average=True)
 sgd.fit(X, np.log1p(y))
 coef_cutoff = np.percentile(np.abs(sgd.coef_), self.percentile_cutoff)
 self.features_to_keep = np.where(np.abs(sgd.coef_) >= coef_cutoff)[0]
 return  X[:, self.features_to_keep]
 我这里简单介绍下SGD
 它是一种梯度随机下降的模型，每次样本估计损失的梯度，模型随着梯度递减(即学习率)增加而更新。
 它有俩种损失函数的惩罚: 平方欧几里德范数L2或对范数L1俩者组合（类似弹性网络）。将模型参数缩减到零向量。
 如果参数更新由于正则化器而超过0.0值，则更新被截断为0.0允许学习稀疏模型并实现线性选择。
 所以，通过官网了解，它通过参数更新让正则化器超过0.0时，可以对稀疏模型实现线性选择。
 
 下面我介绍下如何使用:
 penalty = 'l1' ,因为要带来稀疏性，根据公式这是绝对值，当梯度下降到一定程度时，可能会出现0，所以稀疏。
 loss = 'squared_loss'指的是普通的最小二乘拟合
 alpha 正则化，这里为了迎合稀疏，用个特别小的值3.0e-11
 我们输入一个百分位数，然后求出前百分多少的特征即可，这就是我们上面的意义。
 
 (8)特征合并，也是管道合并，重写官网的。
 
 (9)特征拼接，通过输入不同的特征拼接，去分别拟合不同的模型，最后去调参。
 大概意思跟我在kaggle里写的通过corr拼接一样，做个循环拼接即可，这个也是管道处理的。
 拼接后用TfidfVectorizer处理、
 
 (10)DictVectorizer：
 将特征值映射列表转换为向量。此转换器需要将素名称的映射列表(类似字典的对象)转换为Numpy数组或者scipy.sparse矩阵中的特征值。
 这样才能跟scikit-learn估算器一起使用。
 但请注意，当特征值为string类型时，此转换器仅执行二进制单热编码。如果分类要素表示为数值，例如int，则可以使用DictVector
 izer跟随OneHotEncoder来完成二进制单热编码。
 举例:
 >> from sklearn.feature_extraction import DictVectorizer
>>> v = DictVectorizer(sparse=False)
>>> D = [{'foo': 1, 'bar': 2}, {'foo': 3, 'baz': 1}]
>>> X = v.fit_transform(D)
>>> X
array([[ 2.,  0.,  1.],
       [ 0.,  1.,  3.]])
       
 七.总结这次项目的前期特征工程处理流程
 1.分词，依靠字典，或者正则，或者一些replace过滤噪点。提取词性，
 2.处理nan值。
 3.构造特征
 4.拼接特征
 5.特征提起
 6.分俩种，一种是用CountVectorzer统计词频，然后里面的binary设置为True,这是转换为二值化，为后期我们的神经网络提供效率。
 因为12个神经网络中，有8个是二值化处理的。
 第二种是生成TF-IDF权重矩阵，
 在这里在介绍下这里的用法:
 Counter同理、。
  TfidfVectorizer(
                    max_features=max_feat_descr,  相当于创建一个词典，该词汇表仅考虑语料库中按术语频率排序的最高max_features。
                    ngram_range=(1, 2), 选取至少切1个的词，最多2个词的。
                    token_pattern=TOKEN_PATTERN,  正则表达式的过滤项。
                    dtype=np.float32,
                )
 7.DictVectorizer转化为向量。
 8.稀疏矩阵压缩: csc这个
 9.SanitizeSparseMatrix：可选，提取里面的大值。利用 numpy.nanmax：沿轴返回数组的最大值或最大值，忽略任何NaN。
 最后再用clip(0,max)这种方式截取。
 所以最后是要提取强特征。
 
 
  
  
