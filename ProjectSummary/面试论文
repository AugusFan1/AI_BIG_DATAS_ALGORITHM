    XXX面试官，您好，我是金容明，2013年毕业于哈尔滨理工大学在大庆石油上班，我在2016年4月开始转型IT，我之所以转行，是因为我喜欢这个职业，我最开始在公司了
做的是java开发，大概半年后开始干大数据，主要是搭建过测试集群，用SQL处理数据，最后做的是数据挖掘相关任务，也就是推荐系统。今天，我想把我对推荐系统的理解从
业务层与技术层讲解一下。
    首先，它的架构流程图如下: 开始画图。 然后，它的层级关系是这样，开始画图。 如图所示，我们分俩部分，一部分是我们的商品信息数据，存放在mysql/oracle中。
然后通过sqoop同步到HDFS/HIVE中(HIVE是个数据仓库，假如我们的数据量特别大，需要进行一些业务处理成为新的数据库，我们就需要是用hive.比如银行数据，需要将
原始数据解析成一年的利润额，或者求平均值等等，这些要处理到hive中，然后最后使用时就省去了操作平均值，最大值类似的计算，而是直接调用我们处理好的数据，或者
进行新的业务计算。)。另一个部分是用户点击行为，也就是日志数据，一般是JS文件，然后通过sqoop同步到HDFS中形成块文件，最后这俩种数据通过MR/SPARK到 HBASE 
中，最后我们从HBASE数据导出成.CSV文件，经过推荐算法工程师处理成模型，然后将模型结果的参数，放入到推荐系统里，最后经过redis缓存给前端webservice
展现到结果。
    这中间还涉及到大数据的Flume日志收集，strom/streaming+kafka流式处理，OOZIE调度等，但因为我这次是应聘推荐算法工程师，所以直接跳过了。
    另外，如果你想问我下关于大数据如何处理的，我可以简单说下这些组件应用。redis缓存：高并发读写，海量数据的高效率存储和访问，简单说它是快速的存储访问机制。
它之所以快，是因为它所有数据都是以键值对形式存储，我们知道这种形式是MAP/字典存储，它的时间复杂度是O(1)。同时具有HBASE特点的列簇存储，同一列数据存一起，
进一步提高查询速度。但缺点是存储数据结构化少，但因为是我们最后与前端交互，并不影响。
    接下来，我单独说下推荐算法这块做的工作，主要是业务和技术的结合，推荐算法是一个基于业务的技术，它现在存在的最大问题就是冷启动和电商数据不准确。因为
机器学习是一个基于数据的学科。如果数据本身存在问题，它就不会准确。我说的数据不准确，是指我们获取数据的方式，比如，我们要推荐一个食物，这是物品的标签，
那么它要根据人的标签去推荐。而我们统计用户信息时，部分公司是根据用户第一次访问web日志获取用户的区域IP，然后给用户打了标签，默认为用户是这个区域，然后
用户的特征就是这个地区的IP，但我们知道，中国很多人是本地人去外地人打工的，这样计算会存在很大误差，即使你算法再厉害，最后的模型也不准确，而这种脏数据，
是数据获取时出的问题，推荐算法做模型时很难判别出来，因为它只是个IP地址。所以一个好的推荐系统，首先是大量的前期工作要做的好，简单说就是数据必须准确。
其次就是冷启动问题，这个问题，目前有俩个解决方案，一个是热门推销，将我们公司最想推销出去的商品，进行一个排列，推销给用户，还有就是建立庞大的用户标签系统。
用户第一次来时我们就已经知道他的信息，然后通过它的信息，我们找到相似的物品，进行排序给予推荐。还有一个就是购买大公司的用户数据。
    冷启动问题得到解决，但电商数据是否准确该如何准确？我认为这需要很强的大数据体系，长时间统计用户的IP，然后将这些数据给推荐算法，我们算法工程师，分析
用户数据时，会做一些分析，根据我的分析，我会把这些IP地址按照时间维度去做拼接，然后不同的IP特征+时间特征+用户职业+用户性别做一个四特征拼接，去构建一个强特征。
最后倒入到模型中，我之所以这样做，是因为大部分用户它的IP地址之所以发生改变，是基于他的性别和职业。比如我做的基于用户历史行为的项目中，用户对事件的感兴趣
程度，我会将事件特征里的地域与用户的性别职业+用户的朋友是否喜欢这个事件做拼接，组成强特征，这是基于对现实生活的一个理解，我的思路是一个人，他如果总换
地方，可能他的职业是这类工作，而这种职业是偏向男性多还是女性多我们需要进一步分析，所以才有了这四个特征拼接。而我在我做的模型项目中，是考虑到这个人到底
是否参加这个事件，可能跟这个事件的发生地有关，比如这个人在上海，这个事件在上海，那么他可能去的概率会大点，然后再拼接他的朋友信息是看他朋友是否去，如果
朋友去参加这个事件，或者参加过，那么可能会跟他去说，所以这样去的概率也会大点，最后再根据他的性别是否跟这个事件适合男性或者女性做进一步分析。这就是我们
推荐算法常用的一个技术，特征拼接。这点上，深度学习要做的更好，但是深度学习是黑盒机制，优点是在数据量和特征维度高的情况下，效果强于机器学习，但我们只是
得到推荐效果好，我们不知道如何改进我们的方案，因为它只有结果，没有过程，没有可解释性，而机器学习比如XGB，它可以可视化我们构造的特征，看最后哪些特征
组合的物品适合哪些特征组合的用户，我们基于这个原理，去进行推荐方案的改变。以此达到利润最大化。
    接着我上面提到的特征组合，看深度学习是什么原理做的。这个机制，首先要讲推荐/CTR的鼻祖算法FM,FFM,推荐里也可以说是ALS,隐因子模型。
    FM:
    这个算法是基于我们上述问题中，我们将数据细分化后，会将用户/物品的类别信息都进行One-Hot处理。One-Hot处理是将我们的object类型转化为数字类型。
    变成0 0 0 0 1 0 0 这种类型。我们的长度就是这个类别所有类别的总长度，然后位置上的1就是类别。这样是个时间复杂度为O(N)的计算。pandas中还有get
    _dumpis()有类似功能，但必须要先用skt-learn中的Lanbder进行类别标准化，也就是将类别object类型数据进行数值化处理。这样，最后我们的物品/人的类别
    特征会成为一个海量的稀疏矩阵，这里我先说个个人观点，如果一个公司不需要用深度学习处理，那么我认为在One-Hot编码时可以加个判定条件，
    train_x[i].value_counts()[lambda x:x>30000].index 这个函数是个广告的稀疏判定，推荐同样可以做类似判断，我们将一些业务没用的特征不进行离散化。
    这样可以减少特征维度，也就是说不做这些特征的组合，因为它对我们最终结果影响微乎其微，甚至可能是噪点结果。比如一个人，他就来过一次京东，就买过
    1本书，然后3年之间再没上过，那么我们还需要对他进行特征离散再特征组合处理吗?我认为没必要！当然，我们用深度学习时，由于它的黑和机制，可以自动提取
    有用的特征组合，而且凡是用深度学习处理的公司，都是机器很好的，内存很高，可以承受住海量特征One-HOt后的海量维度。然后再使用FM来解决稀疏问题导致
    的学习模型不准确，它的公式是：这样。 FM我认为是类似SVD矩阵分解，他内部有个隐藏feature,这些隐藏feature是强特征，也就是组成了k个强特征组合。
    假如我们的样本数为N,特征数为M， 那么最后是M*N = M*k*k*N = M*K K*K K*N的拼接。这是我个人看法，这样将时间复杂度降低到了O（kn） 而原来是俩俩拼接
    时间复杂度是2n(n-1)/2 = O(N^2)
    
    FFM:这是由kaggle比赛一个CTR冠军团队三个傻子提出的算法，在FM基础上，他们想到加上阈这个性质。 也就是N个特征有f个阈。
    我先解释下阈，比如城市，性别，就是俩个特征，也是俩个阈。然后公里/小时，英里/小时就是一个阈。 他们的意思是我们2个2个特征组合的是没有依据的，他必须
    在一个阈的情况下进行组合，比如在中国这个阈下，春节+孩子的组合 可以得到这个期间我们推荐糖果。但如果是美国这个阈下，春节+孩子的组合，我们推荐糖果
    得不到更好的效果。因为阈值往往接近特征数，所以时间复杂度为O(K*N^2)
    
    DeepFM:简单说他就是先进入一个嵌入层，然后将特征分开，根据FM得到的二维拼接，分而治之，并行处理。具体我没做过，但这是DCN的前身吧，只是简单看了下
    原理。
    
    基于上面的案例，我们最终得到我们工业中最常用的DCN，这个也是加入了嵌入式+高维拼接的一个适合大部分数据的深度学习网络，下面我讲解下这个网络以及项目
    中如何使用。
    DCN是嵌入层和堆积层开始，接着一个交叉网络和一个与之平行的深度网络，然后是最好的组合层，它结合了两个网络的输出。我先画个图吧:
    然后巴拉巴拉把图说完。 结合项目说:
    首先是嵌入层，我们需要知道我们要找的序号ids:遍历所有类别型数据，然后去重，放入到一个dict里，然后每次range(tc,len(us)+tc)
    tc是我们的初始计数器，初始值为0，而us是我们的去重的类别数据。得到这个ids放入到em里，然后套用网上的DCN网络再进行调试即可。
    tensor的稠密转稀疏。
    indices:我们向量的位置。因为我们处理的文本是第二个位置，所以这里设置为[None,2],代表着处理第二个位置。
    values: 我们输入的向量值，必须是float32
    dense_shape: 我们的维度，为2
    tensor:我们的tensor单位，也就是以上之和。
    这里我再介绍
    几个优化方案:
    
    1.二值化，将float32转为二进制状态。也就是缩小内存32倍，在处理MLP时得到一个不错效果。具体做法是转化为scipy库中的csc压缩矩阵，里面有个参数
    是isbinary,这个是做稀疏矩阵转为稠密矩阵的做法，取代emb嵌入层，但缺点是技术成本比较高，需要设定 value值，偏移指标，shape这些参数，然后调用scripy
    的css,还需要再tensorflow中调用sparse函数来做。
    
    2.设定损失值前面的正则系数，这是我看过kaggle某个大神创建神经网络的技巧，具体是写个函数，判定是L1,L2,
    循环到一定迭代次数后，调用这个来计算不同的损失函数，我认为是为了防止过拟合，基于机器学习的ridge,lasso原理，再把这俩个原理巴拉巴拉说下。
    
    3.设定学习衰退率。
    在tensorflow中，自己带有Adamapitior底层的学习率衰减，我们可以自己设计，在epoch多少时进行衰减。也是为了防止过拟合或者欠你和，这时候你再
    把梯度下降巴拉巴拉讲下。
    
    4.设定激活函数。
    可以自己写函数，然后乘以系数取代relu.比如前面乘以系数，或者a * (x-abs(x)) * 0.5
    
    5.数据是否标准化(这是可选，基于文本推荐使用)
    
    6.设置batch数可变化，也是设定epoch阈值，来进行batch数的改变，也是防止梯度一直消失问题。这些都是实验来推。
    
    深度学习先说这里。
    
    以上都是大公司的要求，然后开始说机器学习，和推荐算法。 结合我们的基于用户历史行为项目说。
    1.关联规则:
    Apriori算法:其实跟文本相似度一样，这是一个基于相似性来求推荐度的算法。
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
