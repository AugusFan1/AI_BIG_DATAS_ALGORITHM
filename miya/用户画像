一. TF-IDF思想与实现
1.思想:某一个主题频率高+其他主题频率低=>该词很重要,用于因子加权。也可以将TF和IDF分别进行其他方面的计算。

二. 人群划分
1.






















一.用户偏好度特征与算法详解
下面基本都用log来算。

1.偏好度与时间因子相关知识
(1).特征因子背后的数学理论
牛顿之前就发明了，用户的喜好是根据时间的延长而降低，所以发明了如下公式:
牛顿冷却定律: 本期温度 = 上一期温度 * exp(-(冷却系数) * 间隔的小时数)
冷却系数是超参数， 假设一个文章初始为100分，24小时候为1分，那么冷却系数为0.192. .这个值你要根据业务场景来设定，如果想放慢温度，就降低。
这里的值可以理解各个场景，用户偏好度也可以这么理解。 
还有一个是时间衰减系数(也是冷却系数): -0.0023 *  距今天数，30天刚好衰减一半。

(2).冷却系数公式证明。
根据上面中24小时变为1分，我们可以理解为缩小100倍。时间为24
=> exp(-(冷却系数) * 24) = (本期温度/上一期温度) = 0.01 = >
-(1/log(0.01)) *24 = 冷却系数 =>冷却系数=0.192

2.权值的分配
我们计算时，将这些特征直接乘以相关系数。
比如购买过 * 3. 点击过 *1 点击后又收藏 * 2

3. 购买次数得分
这里就用到冷却系数，我们这里叫时间衰减。
简单说购买次数*(时间衰减*时间) 等于这个人当前对这个物品的喜欢程度，这个当一个强特征。
i = 1....N表示当前时间到起始时间的天数，Li表示第i天的购买次数，表示第i天距离
当前时间的天数，然后根据公式推导出:

4. 购买规律性得分: 过去一段周期内用户购买物品向量归一化后的商 + 一个小的偏移量。
Ex: 过去6个月该用户 购买女装类别次数为(0.0,1,1,0) ,商值为 -0.5 * log(0.5) - 0.5 * log(0.5)
购买手机(0,0.1,0.0)商值为1/6 * log(1/6) - 5/6 * log(5/6) 看出购买女装更加有规律性。

5.频次: 类似TF原理。 用户某标签个数 / 用户标签总数

6.多样性得分 = 过去一段周期内该类被购买个数取对数 + 一个小的偏移量

7. 热度降权，IDF思想，所有用户标签购买之和/某个标签用户购买之和。
简单说，所有用户的类别数目和为100. 总共5个类别，A类别总和为30. 
那么A类别我们可以理解为 30 / 100 =0.3为当前热度，随着时间推移，我们会对这个类别
的热度降低权重，因为新类别的诞生，会影响老类别的销量，所以降低权重更符合正常逻辑。

也可以反过来说，比如，这个公式里是分子为总和，所以理解为冷门程度，如果客户购买这类物品，这类客户在买这类物品的冷门的权重系数要增加。

8.画像权重计算方法:
总结: 
用户偏好度 = 行为权值 *  行为次数 * 时间衰减 * 购买规律性 * 购买多样性。

9。TF-IDF。
但这个太简单，不太好。

四.常用特征方法:
1.根据连续型数据改变相应的类别型数据。
比如:类别型数据性别本身是男女，根据年龄可以增加到小孩，老人，男，女。

2.类别与类别之间的相互改变
比如:根据中英文特点，将名字含有什么的定义为男，女等。
Jack -> 男。 mary ->女。
PS:名字缺失值用other取代

3.连续数据区间划分成类别特征

4.连续特征累加成新特征: 某类连续特征的累加。 比如父母带小孩等等

五. 用户画像标签的权重核心: EdgeRank
原文意思是 
∑亲密度*边的权重*新鲜程度 =》
∑构造的特征系数*该特征的权重*时间因子(随着时间增加兴趣度降低)
https://en.wikipedia.org/wiki/EdgeRank
https://www.shujike.com/blog/12009
https://www.cnblogs.com/peizhe123/p/5027619.html


六. 
一.前沿
大数据时代将"人"数据化。
数据来源:
1用户与计算机应用交互产生的数据，比如淘宝。
2.穿戴可移植设备，比如走路的步数。
3.人与人之间设备，比如发短信。

我们必须将这些内容转化成计算机能读懂的数据(二进制，比特)，再进行建模。

二.什么是用户画像。
对现实世界中用户的数学建模。
1.是描述用户的数据，符合特定业务需求对用户的形式化描述,源于现实,高于现实。
2.是分析挖掘用户尽可能多的数据信息得到的，源于数据,高于数据。

宗旨:大数据+观察。

三.用户画像作用




1.用户精细化运营:
(1)优惠券如何发放
(2)广告信息推送给谁
(3)流失预警
(4)品类转新

2.商户精细化运营支持
(1)拉新策略
(2)活动精准推送。
(3)定价策略

3.个性化
(1)推荐:团单推荐，菜品推荐,店铺推荐。
(2)排序

4.大数据报告
(1)电影大数据报告
(2)餐饮大数据报告
(3)决策分析支持

5.趋势预测
(1)票房预测
(2)销量预测


四,标签标示方法
标签是某一种用户特征的符号表示。
1.标签体系
(1)化整为零: 标签是某一种用户特征的符号表示。相当于类。这个人属于工人，那个人属于学生。

(2)化零为整:用户画像是一个整体,各个维度不孤立,标签之间有联系。
用户画像可以用标签的集合来表示。

2.数学描述
标签是特征空间的维度。
(1)化整为零
每个标签都是特征空间中的基向量。
(2)化零为整
基向量之间有关联,不一定是正交的。
用户画像本质就是特征空间的高维向量。

3.用户画像系统的挑战
(1)记录和存储亿级用户的画像(大数据解决)
(2)支持和扩展不断增加的维度和偏好。(大数据解决)
(3)毫秒级的更新。(spark和hbase解决)
(4)支撑个性化推荐,广告投放和精细化营销等产品。(我们推荐算法和建模能力)

四.系统流程


1.明确问题:分类问题还是回归问题?用户是否会流失?下个月的销量?
2.数据预处理: 第三方接口数据，离线数据，数据都不规范，可能不按照自己公司规范，比如男女,有的用1/0或者N/F这类。年龄边界出现-1，999这些内容。
3.特征工程
4.模型算法
5.产出:个性化推荐/广告等等。


五.用户画像处理流程
1.明确问题和了解数据
(1)明确问题和了解数据
追求需求和数据的匹配:寻找用户数据是否真实，有无欺诈，风险评估等。
明确需求
分类:已知有历史数据来分类。监督。

聚类:不知道数据分成几个类，根据制定的值，化成几个类，出现这个就划分到那个。
基本就用k-means算法。比如将文章分成多少个簇。比如:包含大数据,hadoop等规划到大数据簇，NBA,欧冠划分到体育类。

推荐和其他:基于协同过滤还是内容。

其他:比如回归问题:通过今天天气来预测出明天天气，通过当前销量下个月销量，用户流失比例。

(2)数据的规模,重要特征的覆盖度等。
是否存在欺诈，逾期的维度，会不会存在没有逾期等。

2.数据预处理
(1)数据集成:各种数据来源（MySQL，.csv,xml）统一作为hbase表里或者mapreduce的输入数据生成输出数据。
(2)数据冗余，数值冲突:
(3)数据抽样:来源数据特别大时用，注意重要特征的覆盖度。
(4)数据清洗,缺失值处理,噪声数据:
噪声数据: 指出现在某变量上的随机误差或变异。 不一致数据（inconsistentdata） 指在数据库中属性值相同的事例分属于不同的类的情况。

3.特征工程
1.数据和特征决定了机器学习的上限,而模型和算法只是逼近这个上限而已。
特征:对所需解决问题有用的属性。
特征的提取,选择和构造。
(1)针对所解决的问题选择最有用的特征集合
(2)通过相关系数等方式来计算特征的重要性。
     人工筛选
     有些算法输出特性:Random Forest(随机森林。)
     维度过多,PCA自动降维。

2.特征提取
(1)业务日志
(2)WEB公开数据抓取
(3)第三方合作

3.特征处理
(1)特征清洗
(2)特征预处理:值处理,特征选择,特征组合,降维。
(3)商业加工。

4.特征监控
(1)指标:时效性,覆盖率和异常值
(2)可视化&预警。

5.模型与算法


平滑:对稀疏问题进行处理，样本不够，出现为0这些情况。
归一化: 降低难度，将数据变为0和1这类方法。

5.应用举例:



6.用户画像系统架构







HBASE是数据建模部分，当然数据建模也可能在HDFS上。数据建模用到mapreduce和spark.然后为了快速展现用redis.

五.案例:
性别预测
1.规划计划
（1）.属于哪一类?
分类,聚类,推荐还是其他?
回答: 分类，而且是二分法。
（2）.数据规模。
如果不大，用python
如果很大，用分布式架构处理，spark/mapreduce
存放在HDFS/HBASE
（3）.问题假设
对数据要进行查看，预估。
数据是否满足所解决问题的假设?(男女行为不同)

2.数据预处理


三.特征工程




停用词: 停用词是指在信息检索中，为节省存储空间和提高搜索效率，在处理自然语言数据(或文本)之前或之后会自动过滤掉某些字或词，这些字或词即被称为Stop Words(停用词)。

步骤4：算法和模型
1.选择算法和模型考虑的因素
(1)训练集的大小: 8:2
(2)特征维度大小: 高/低维
(3)解决问题是否线性可分:利用线性函数/平面/直线区分。考虑用逻辑回归
(4)所有特征是独立吗? 考虑特征是否有相关性。
(5) 是否考虑过拟合问题？
如果是逻辑回归会出现欠拟合问题: 本身应该出现在线的上面，实际却在下面。
过拟合:过多的拟合数据，模型过于敏感，判断一些数据产生问题，本质是参数过大。
(6)对于性能要求? 时效。

奥卡姆剃刀原理: 尽可能选择用的多，简单的算法，比如逻辑回归。

LR:逻辑回归
(1)只要认为问题是线性可分,就采用LR,非线性的也可以想办法转化成线性。
(2)模型比较抗噪,而且可以通过L1，L2范数来做参数选择。(范数是用来计算距离。L2是欧式，常用。L1是曼哈顿)
(3)效率高,可以用于数据特别大的场景。
(4)很容易分布式实现。

Ensemble方法(集成方法)
根据训练集训练多个分类器,然后综合多个分类器的结果,做出预测。取平均结果。

分为bagging 和 bossting,具体看以前笔记视频。


四.用户画像的系统应用
1.用户信用等级分析
聚类问题。k-means, 随机森林。
出账信息，入网时间，入网套餐价格，预付/后付。
月流量消耗。号码归属地

支付记录 消费频次 消费金额 支付成功率 -》 K-means聚类 分簇，随机森林。

2.大数据营销应用。
根据画像的校园和偏好标签做营销。
比如吃货排行——>零食销量。
颜值排行-->化妆品销量。

用户偏好画像的标签是通过用户的搜索,浏览，购买等所有站内行为计算而来，针对标签的监控，可以体现用户的喜好和关注度迁移变化。比如用户只是浏览或者用户是真的喜欢，通过数据来体现出来。

3.用户流失预警
属于分类问题，流失/不流失。
特征:活跃度，登陆情况，下载情况，预装机情况，机型。
潜在流失用户:针对可能流失的用户做PUSH推广活动。
相对算法: GBDT

4.潜在用户分析
算法 Logic Regression 逻辑回归，线性回归。
通过计算机来特征分解，模型训练，推算出潜在群体预测、
全量进行用户画像库。
以在应用或者游戏中产生转化的这些用户作为训练正样本,结合用户特征进行模型训练。
从用户画像库中筛选出潜在的用户群体，推荐给CP,通过PUSH做相关的营销活动。

5.异常检测与分析-离群点分析
(1)异常检测: 异常离群点:比如一个用户在半夜在一个小超市刷了很多钱。
(2)刷机诊断
基于规则 (IF ELSE) :依赖经验,准确度低。
基于模型:
 收入增幅,通话短信,省市分布，好吗特征。。判断是否刷机?




