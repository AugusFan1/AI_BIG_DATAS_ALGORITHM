# -*- coding: utf-8 -*-
'''
@author: 金容明
@file: tf_lstm_rfm.py
@time: 2019/05/23 11:27
@help: RFM预测模型训练阶段
'''
###引入第三方模块###
import os
import re 
import sys
import time
import datetime
import argparse
import numpy as np
import pandas as pd
import multiprocessing
import tensorflow as tf
import matplotlib.pyplot as plt

from random import shuffle
from tensorflow.contrib import rnn


###定义设置LSTM常量###

# 获取buckets的动态参数
FLAGS=None   

def Config():
    parser = argparse.ArgumentParser()
    parser.add_argument('--buckets', type = str, default = '', help = 'bueckts桶')
    parser.add_argument('--checkpointDir', type = str, default = '',help = '模型保存路径的默认路径')
    parser.add_argument('--num_classes', type = int, default = 2,help = '预测类别总数')
    parser.add_argument('--max_steps', type = int, default = 10000, help = '训练集最大迭代次数')
    parser.add_argument('--learning_rate', type = float, default = 0.01, help = '学习率' )
    parser.add_argument('--learning_rate_dec',type = float, default = 0.6, help = '学习下降率,指数级别下降。' )
    parser.add_argument('--keep_prob', type = float, default = 0.6, help = 'DropOut训练时最大保留率')
    parser.add_argument('--keep_prob_end', type = float, default = 0.5, help = 'DropOut模型保存时最大保留率')
    parser.add_argument('--time_train_step', type = int, default = 90, help = '训练集的最大天数')
    parser.add_argument('--time_label_step', type = int, default = 1, help = '预测最大天数,我们这次是第30天，所以是1')
    parser.add_argument('--feature_col', type = int, default = 3, help = 'feature维度')
    parser.add_argument('--feature_num', type = int, default = 2, help = 'feature数量')
    parser.add_argument('--model_id_num', type = int, default = 4, help = '模型ID数量')
    parser.add_argument('--feature_col_label', type = int, default = 1, help = 'label的feature维度,默认为1')
    parser.add_argument('--layer_num', type = int, default = 2, help = '网络层数')
    parser.add_argument('--cell_num', type = int, default = 64, help = '神经元个数')
    parser.add_argument('--output_num', type = int, default = 1, help = 'label维度')
    parser.add_argument('--train_feature_start', type = int, default = 3, help = '训练集原始特征维度数')
    parser.add_argument('--l2_normal', type = float, default = 0.0001, help = 'L2正则项')
    parser.add_argument('--max_num', type = float, default = 100, help = '最大迭代次数')
    parser.add_argument('--batch_size',type = int, default = 512, help = '每次迭代数量')
    parser.add_argument('--loss_min',type = float, default = 0.0001, help = '训练集和验证集的最小损失值')
    parser.add_argument('--normal_epsilon',type = float, default = 0.01, help = '标准化中防止除以0的情况所留的小数')
    parser.add_argument('--thread_num', type = int, default = 5, help = '读取数据的线程数量')
    parser.add_argument('--is_training', type = bool, default = True, help = '读取数据是否为训练集，默认为True,False为验证集')
    parser.add_argument('--train_path', type = str, default = 'oss://bucketjrm2.oss-cn-shanghai-internal.aliyuncs.com/data_temp/train*',help = '训练集输入路径')  # 后续需要去掉只剩下train*
    parser.add_argument('--decay_num', type = int, default = 1000, help = '每多少次学习率更新')
    parser.add_argument('--test_path',type = str,default = 'test*',help = '测试集输入路径')
    parser.add_argument('--model_out_path', type = str, default = 'oss://bucketjrm2.oss-cn-shanghai-internal.aliyuncs.com/tensorflow_model/',help = '模型输出路径')
    parser.add_argument('--model_in_path', type = str, default = 'oss://bucketjrm2.oss-cn-shanghai-internal.aliyuncs.com/tensorflow_model/',help = '模型输入路径')
    parser.add_argument('--total_num', type = int, default = 50000, help = '总数据输入数量')
    parser.add_argument('--train_num', type = int, default = 40000, help = '一次获取的训练集输入数量')
    parser.add_argument('--valid_num', type = int, default = 5000, help = '验证集输入数量')
    parser.add_argument('--test_num', type = int, default = 3600000, help = '模型输出数量')
    parser.add_argument('--min_num', type = int, default = 0, help = '最小数值')
    parser.add_argument('--min_after_dequeue', type = int, default = 100, help = '最小出队数,至少达到多少数时，队列才出队。')
    parser.add_argument('--is_single', type = bool, default = True, help = '是否开启单卡多节点/单节点')
    parser.add_argument('--is_distribute', type = bool, default = True, help = '是否开启分布式')
    parser.add_argument('--python_thread', type = int, default = 2, help = 'python的进程数,后续优化需要用到')
    parser.add_argument('--device', type = str, default = 'GPU', help = 'python为GPU还是CPU')
    parser.add_argument('--workers', type = int, default = 1, help = '分布式的节点数')
    parser.add_argument('--gpu_num', type = int, default = 2, help = 'GPU个数,在pai里单机最多为2')
    parser.add_argument('--data_result',type = str,
                        default = 'oss://bucketjrm2.oss-cn-shanghai-internal.aliyuncs.com/data_result/',
                        help = '模型结果输出路径')



    FLAGS, _ = parser.parse_known_args()
    return FLAGS

## 加载数据

def load_data():
    # 读取OSS数
    train_file_path = tf.gfile.Glob(os.path.join(FLAGS.buckets,FLAGS.train_path))

    # 创建tf的reader对象
    reader = tf.TextLineReader()
    # 将OSS数据转成序列格式
    file_name_queue_train = tf.train.string_input_producer(train_file_path)
    # 懒加载序列里的value值。
    key_train,value_train = reader.read(file_name_queue_train)

     # 初始化特征列,这行是double类型
    record_defaults_train = [["1.0"],["1.0"],["1.0"],["1.0"],["1.0"],["1.0"],["1.0"]] 

    col1, col2, col3, col4, col5, col6, col7 = tf.decode_csv(value_train, record_defaults=record_defaults_train)

    # 一次读取大量数据,毕竟是测试集，我们不需要batch读取
    # 注意，该方法返回的是个list，后续如果用矩阵计算，需要转成numpy np.array(list)
    # 我们如果想确定随机打乱数据，可以直接用: 
    feature_train = tf.train.shuffle_batch([col5, col6, col7],batch_size=FLAGS.total_num, num_threads=FLAGS.thread_num, capacity=FLAGS.total_num, min_after_dequeue = FLAGS.min_after_dequeue)  
    init = tf.global_variables_initializer()
    with tf.Session() as sess:  
        sess.run(init)
        
        coord = tf.train.Coordinator()
        threads = tf.train.start_queue_runners(sess = sess,coord = coord)   
        
        data = sess.run([feature_train])
        data = np.array(data)
        # 生成的是三维度，需要转成二维,注意这个二维是跟原维度相同即可。
        data = data.reshape(FLAGS.train_feature_start,FLAGS.total_num).T     
                    
        # 结束读取数据进程
        coord.request_stop()
        coord.join(threads)
        
    return data


## 处理训练集数据

def get_train_data(data_train,batch_size,batch_size_first,batch_size_end,train_path,is_training = True):
    data_temp = data_train
    
    # 如果是验证集,每次随机打乱数组进行抽取，用bagging本质，防止过拟合
    # 如果是训练集,则按照step依次添加
#     if is_training == False:
#         data_temp = data_train[batch_size_first:batch_size_end,:]
#     else:
#         np.random.shuffle(data_train)
#         data_temp = data_train[:batch_size,:]
        
    data_temp = data_train[batch_size_first:batch_size_end,:]
    X_train = np.zeros(shape = (1))
    y_train = np.zeros(shape = (1))

    X_train = np.delete(X_train,0,axis = 0)
    y_train = np.delete(y_train,0,axis = 0)
    
    # 训练集
    for i in data_temp:
        
        list1 = list(i[0].replace('[','').replace(']','').replace('"','').split(','))
        aa_num = len(list1)
        aa = [ float(list1[t1]) for t1 in range(aa_num) if t1 % (FLAGS.feature_col + 1) != 1 ]
        
        list1 = list(i[1].replace('[','').replace(']','').replace('"','').split(','))
        aa_num = len(list1)
        bb = [ float(list1[t2]) for t2 in range(aa_num) if t2 % (FLAGS.feature_col + 1) != 1 ]
      
        aa = aa + bb
        cc = [float(i) for i in i[2].split(',')]
             
        X_train = np.append(X_train,aa).reshape(-1,FLAGS.time_train_step * FLAGS.feature_col)
        y_train = np.append(y_train,cc).reshape(-1,FLAGS.time_label_step * FLAGS.feature_col_label)
    
    return X_train,y_train


## 分布式初始化类

class ConfigGPU(object):
    def __init__(self):
        self.gpu_list = range(FLAGS.gpu_num)
        self.CUDA_VISIBLE_DEVICES = ','.join([str(x) for x in self.gpu_list])

from tensorflow.python.ops import init_ops
## 构造分布式的LSTM类,后续扩展后再研究模型融合的方案

class LstmModelMultiClassfi(object):
    def __init__(self,FLAGS):
        with tf.device('/cpu:0'):
            print('分布式LSTM模型初始化开始...')
            self.batch_size = FLAGS.batch_size 
            self.keep_prob = FLAGS.keep_prob
            self.gpu_list = [i for i in range(FLAGS.gpu_num)]
            self.time_train_step = FLAGS.time_train_step
            self.feature_col = FLAGS.feature_col
            self.output_num = FLAGS.output_num
            self.num_classes = FLAGS.num_classes
            self.layer_num = FLAGS.layer_num
            self.cell_num = FLAGS.cell_num
            self.l2_normal = FLAGS.l2_normal
            self.normal_epsilon = FLAGS.normal_epsilon
            self.decay_num = FLAGS.decay_num
            self.learning_rate_dec = FLAGS.learning_rate_dec
            self.checkpointDir = FLAGS.checkpointDir
            self.model_out_path = FLAGS.model_out_path
            self.total_num = FLAGS.total_num 
            self.max_num = FLAGS.max_num
            self.loss_min = FLAGS.loss_min
            # 学习率我们用此方法设置为动态
            self.learning_rate = FLAGS.learning_rate
            
            print('分布式LSTM模型初始化结束...')
            
    
    # 正则化
#     def weight_variable(self, shape, l2):
#             if l2 is not None or l2 != '':
#                 with tf.variable_scope(tf.get_variable_scope(), reuse= tf.get_variable_scope().reuse) :   
#                     regularizer = tf.contrib.layers.l2_regularizer(l2)
#                     weight_decay = tf.get_variable('weight_loss',shape,dtype = tf.float32,initializer = tf.contrib.layers.xavier_initializer(),trainable=True)
#                     tf.add_to_collection('losses', regularizer(weight_decay))
#                     return weight_decay
#             else:
#                 return
            
    # 梯度 
    def average_gradients(self,tower_grads):   
        average_grads = []
        for grad_and_vars in zip(*tower_grads):
            grads = [g for g, _ in grad_and_vars]
            grad = tf.stack(grads, 0)
            grad = tf.reduce_mean(grad, 0)
            grad_and_var = (grad, grad_and_vars[0][1])
            average_grads.append(grad_and_var)
        return average_grads

            
            
    # 创建网络对象 
    def build_model(self,batch_size,keep_prob,x,y,scope,num_classes,layer_num,time_train_step,feature_col,cell_num,l2_normal,normal_epsilon):
        # tf.reset_default_graph()   
        with tf.variable_scope(tf.get_variable_scope(), reuse= tf.AUTO_REUSE) :   
            input_x = [x[i] for i in range(batch_size)]
            input_y = [tf.one_hot(y[j],depth = num_classes) for j in range(batch_size)] 
        
            W = {
                 'in' : tf.get_variable('w_in',shape = [feature_col,cell_num],
                                         dtype = tf.float32,
                                         initializer = tf.contrib.layers.xavier_initializer(),trainable=True),
                 'out' : tf.get_variable('w_out',shape = [cell_num,num_classes],
                                         dtype = tf.float32,
                                         initializer = tf.contrib.layers.xavier_initializer(),trainable=True)
                }
            bias = {
                      'in' : tf.get_variable('b_in',shape = [cell_num], initializer=init_ops.constant_initializer(0.1) ,trainable=True),
                      'out' :tf.get_variable('b_out',shape = [num_classes], initializer=init_ops.constant_initializer(0.1) ,trainable=True)
                    }
#             gamma = tf.Variable(tf.ones([time_train_step * feature_col]))
#             beta = tf.Variable(tf.zeros([time_train_step * feature_col]))
#             lstm_mean = tf.Variable(tf.zeros([time_train_step * feature_col]), trainable = False)
#             lstm_variance = tf.Variable(tf.ones([time_train_step * feature_col]), trainable = False)
            
            gamma = tf.get_variable('gamma_in',shape = [time_train_step * feature_col],initializer=init_ops.constant_initializer(1),trainable=True)
            beta = tf.get_variable('beta_in',shape = [time_train_step * feature_col],initializer=init_ops.constant_initializer(0),trainable=True)
            lstm_mean = tf.get_variable('lstm_mean_in',shape = [time_train_step * feature_col], initializer=init_ops.constant_initializer(0),trainable=True)
            lstm_variance = tf.get_variable('lstm_variance_in',shape = [time_train_step * feature_col],initializer=init_ops.constant_initializer(1),trainable=True)
            
            input_x_normal = tf.nn.batch_normalization(input_x, lstm_mean, lstm_variance, beta, gamma, normal_epsilon)
            
            # input_x_normal = input_x
            # 通过reshape将输入的input_x转化成2维，-1表示函数自己判断该是多少行，列必须是feature_col
            input_x_r = tf.reshape(input_x_normal,[-1,feature_col])                
            
            # relu和dropout操作
            input_x_in_r = tf.nn.relu(tf.matmul(input_x_r,W['in']) + bias['in'])
            input_relu = tf.nn.dropout(input_x_in_r, keep_prob)
            
            # 即LSTM接受：batch_size个，time_train_step行cell_num(神经元个数)列的矩阵
            input_rnn = tf.reshape(input_relu, [-1, time_train_step, cell_num])
            
            # 定义一个带着“开关”的LSTM单层
            def lstm_cell(cell_num,keep_prob):
                cell = rnn.LSTMCell(cell_num, activation = tf.nn.softsign, reuse = tf.get_variable_scope().reuse)
                return rnn.DropoutWrapper(cell, output_keep_prob = keep_prob)
            
            # 拼接网络层 
            lstm_layers = tf.contrib.rnn.MultiRNNCell([lstm_cell(cell_num,keep_prob) for _ in range(layer_num)], state_is_tuple = True)
            # 初始化LSTM网络,以下是固定格式
            init_state = lstm_layers.zero_state(batch_size, dtype = tf.float32)
            # 动态组合
            outputs, state = tf.nn.dynamic_rnn(lstm_layers, inputs = input_rnn, initial_state = init_state, time_major = False)
            h_state = state[-1][1]
            # 我们预测的结果的概率
            logits = tf.matmul(h_state,W['out']) + bias['out']
            # 我们预测的结果
            pre_d = tf.argmax(logits,1)
            # 我们的input_y 是 list类型,与 pre_d 必须要转成tensor才可以。 
            target = tf.argmax(tf.reshape(input_y,[-1,num_classes]),1)
            corr = tf.cast(tf.equal(pre_d,target),tf.float32)
             # 统计正确总数
            correct_num = tf.reduce_sum(corr)
            # 统计acuuary值
            accuracy = tf.reduce_mean(corr)
            
            cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = input_y,logits = logits)) # 原始LOSS
            regularization_loss = l2_normal * tf.reduce_sum([tf.nn.l2_loss(W['in'])])  # 正则LOSS
            loss = cross_entropy + regularization_loss 
            
            return loss,logits,pre_d,target,correct_num,accuracy
    
    # 训练模型
    def train_lstm(self):
        self._graph = tf.Graph()  # 构建在一个图中
        with self._graph.as_default():
            print('分布式LSTM模型开始构建...')
            
            # 学习率我们用此方法设置为动态
#             self.learning_rate = FLAGS.learning_rate
            
            global_step = tf.get_variable('global_step', [],initializer=tf.constant_initializer(0), trainable=False)
            self.learning_rate = tf.train.exponential_decay(self.learning_rate,global_step,self.decay_num, self.learning_rate_dec,staircase=True) 
            self.models = []   
            optimizer = tf.contrib.opt.LazyAdamOptimizer(self.learning_rate)
            
            with tf.variable_scope(tf.get_variable_scope()):
                for gpu_id in self.gpu_list:
                     with tf.device('/gpu:%d' % gpu_id):        # 初始化第一个设备
                        print ('gpu:%d...'% gpu_id)
                        with tf.name_scope('tower_%d' % gpu_id) as scope:  # 初始化命名空间
                            # with tf.variable_scope('gpu_v', reuse=gpu_id > self.gpu_list[0]):
                            x = tf.placeholder(tf.float32,[None,self.time_train_step * self.feature_col])
                            y = tf.placeholder(tf.int32,[None,self.output_num])
                            loss,logits,pre_d,target,correct_num,accuracy = \
                            self.build_model(self.batch_size,self.keep_prob,x,y,\
                            scope,self.num_classes,self.layer_num,self.time_train_step,self.feature_col,self.cell_num,self.l2_normal,self.normal_epsilon)
                            # 开启共享
                            tf.get_variable_scope().reuse_variables()
                            # 开始计算梯度
                            grads = optimizer.compute_gradients(loss)
                            self.models.append((loss,correct_num,accuracy,grads))
        
            loss,correct_num,accuracy,grads = zip(*(self.models))
            
            self.loss = tf.reduce_mean(loss)
            self.correct_num = tf.reduce_mean(correct_num)
            self.accuracy = tf.reduce_mean(accuracy)
            self.train_op = optimizer.apply_gradients(self.average_gradients(grads))
            
            
            # init = tf.global_variables_initializer()
            ckpt_path = os.path.join(self.checkpointDir, self.model_out_path)
            saver = tf.train.Saver()
            
        config = tf.ConfigProto()
        config.gpu_options.allow_growth = True     
        with tf.Session(config = config) as sess:   
            try:
                module_file = os.path.join(self.checkpointDir, self.model_out_path)
                print('模型参数路径: ',module_file)
                saver.restore(sess, module_file)
                print ("成功加载模型参数")        
            except:
                #如果是第一次运行，通过init告知tf加载并初始化变量
                print ("没有加载成功，进行初始化操作,开始训练模型...")
                sess.run(tf.global_variables_initializer())
            
            # 训练集读取
            NUM = self.total_num // self.batch_size
            for z in range(self.max_num):
                # 加载数据
                data = load_data()
                try:
                    for i in range(NUM):              
                        # 读取训练数据集
                        train_x, train_y = get_train_data(data_train = data , batch_size = self.batch_size ,batch_size_first = 0 + i * self.batch_size , batch_size_end = self.batch_size + i * self.batch_size, train_path = FLAGS.train_path, is_training = FLAGS.is_training)
            
                        _, train_loss, correct_num_d,accuracy_d = sess.run([train_op,loss,correct_num,accuracy],feed_dict = {x: train_x, y: train_y})
                                   
                        if i % 20 == 0:
                            print("epoch: {0}, step: {1},train_loss: {2},correct_num: {3},accuracy: {4".format(z + 1,i+1,train_loss,correct_num_d,accuracy_d))
                            saver.save(sess,ckpt_path)
                        if train_loss < self.loss_min:
                            print ("训练集均已训练完毕,开始保存模型...")
                            save_path = saver.save(sess,ckpt_path)
                            print('模型路径为: ',save_path)
                            return 
                        
                
                        # 每经历一次遍历,模型保存一次.
                    saver.save(sess,ckpt_path)
    
                    # 防止迭代超出范围,我们设定StopIteration异常,如果执行下面步骤说明迭代已经到了上限,此时我们执行最后一次，然后开始保存模型。
                except StopIteration:
                    print ("训练集均已训练完毕,开始保存模型...")
                    save_path = saver.save(sess,ckpt_path)
                    print('模型路径为: ',save_path)
                    break  
        
        print('模型训练结束...')
        
        return self
            
                        
 # 主函数编写,因为pai里目前不支持调用python主参数,在这里我用FLAGS取代写成定式,后续到其他平台可以修改回来。
def main(is_single,is_distribute):
    FLAGS = Config()

    if is_distribute == FLAGS.is_distribute and is_single == FLAGS.is_single:
        lstm_classfi_model = LstmModelMultiClassfi(FLAGS)
        lstm_classfi_model.train_lstm()
#     if is_distribute != FLAGS.is_distribute and is_single == FLAGS.is_single:
#         return 
#     if is_distribute != FLAGS.is_distribute and is_single != FLAGS.is_single:
#         return 
    
    
if __name__ == '__main__':  
    # tf.app.run(main = main(True,True))
    # tf.app.run()
    main(True,True)

    
            
    
    
