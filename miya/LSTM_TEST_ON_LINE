# -*- coding: utf-8 -*-
'''
@author: 金容明
@file: tf_lstm_rfm.py
@time: 2019/05/23 11:27
@help: RFM预测模型测试阶段
'''
###引入第三方模块###
import os
import re 
import sys
import time
import argparse
import numpy as np
import pandas as pd
import multiprocessing
import tensorflow as tf
import matplotlib.pyplot as plt

from random import shuffle
from tensorflow.contrib import rnn


###定义设置LSTM常量###

# 获取buckets的动态参数
FLAGS=None   


## 加载数据
def load_data(test_path):
    # 读取OSS数
    test_file_path = tf.gfile.Glob(os.path.join(FLAGS.buckets,FLAGS.test_path))
    
    # 创建tf的reader对象
    reader = tf.TextLineReader()
    # 将OSS数据转成序列格式
    file_name_queue_test = tf.train.string_input_producer(test_file_path)

    # 懒加载序列里的value值。
    key_test,value_test = reader.read(file_name_queue_test)

     # 初始化特征列,这行是double类型
    record_defaults_test = [["1.0"],["1.0"],["1.0"],["1.0"],["1.0"],["1.0"],["1.0"]]  

    t1, t2, t3, t4, t5, t6, t7 = tf.decode_csv(value_test, record_defaults=record_defaults_test)

    
    # 前三个字段时表示唯一用户,我们这里第一步先不处理，最后一个时特征列,pt_day是时间。我们把除了前三个和最后一个外其他拼接成特征列.
    feature_test = tf.concat([[t1], [t2], [t3], [t4], [t5], [t6], [t7]],0)

    # 我们需要将数据一步步喂给DataFrame
    init = tf.global_variables_initializer()    
    with tf.Session() as sess:    
        sess.run(init)
        coord = tf.train.Coordinator()
        threads = tf.train.start_queue_runners(sess = sess,coord = coord)       
        
        df_test = sess.run([feature_test])
        for j in range(FLAGS.test_num - 1):
            df_test = np.append(df_test,sess.run([feature_test]),axis = 0)   
          
        # print(df_train.shape)
        # print(df_valid.shape)
        # print(df_test.shape)
        print('读取数据结束...')
        print(df_test.shape)
        coord.request_stop()
        coord.join(threads)
    return df_test


## 处理测试集数据
def get_test_data(data_test,batch_size_first,batch_size_end,test_path):
    # 这里必须按照严格顺序执行，后续要与ID拼接
    data_test = data_test[batch_size_first:batch_size_end,:]
#     print(data_test)
#     print(data_test.shape)
    X_test = np.zeros(shape = (1))
    y_test = np.zeros(shape = (1))

    X_test = np.delete(X_test,0,axis = 0)
    y_test = np.delete(y_test,0,axis = 0)
    
        # 训练集
    for i in data_test:
        list1 = list(i[4].replace('[','').replace(']','').replace('"','').split(','))
        aa = []
        n = 0
        for t1 in list1:
            n = n + 1
            if n % 4 == 1:
                continue 
            aa.append(float(t1))
        # print(len(aa))
        n = 0 
        
        list1 = list(i[5].replace('[','').replace(']','').replace('"','').split(','))
        bb = []
        for t2 in list1:
            n = n + 1
            if n % 4 == 1:
                continue 
            bb.append(float(t2))
        
        aa = aa + bb
        n = 0
        cc = [float(i) for i in i[6].split(',')]
        # print(len(aa))
        
#         list1 = list(i[2].replace('[','').replace(']','').replace('"','').split(','))
#         cc = []
#         for t3 in list1:
#             n = n + 1
#             if n % 4 == 1:
#                 continue 
#             cc.append(float(t3))

        X_test = np.append(X_test,aa).reshape(-1,FLAGS.time_train_step * FLAGS.feature_col)
        y_test = np.append(y_test,cc).reshape(-1,FLAGS.time_label_step * FLAGS.feature_col_label)
        data_test = data_test[:,:4]
        # print(data_test.shape)
    return data_test,X_test,y_test




## 开始设计神经网络
def train():
    '''
    所有LSTM操作都在此进行,目前都放在一个图上。
    '''
  

    # 设置tf按需使用GPU资源，而不是有多少就占用多少checkpointDir
    g1 = tf.Graph()
    with g1.as_default():
        
        test = load_data(FLAGS.test_path)
        
        # 编写正则化函数
        def weight_variable(shape, l2):
            if l2 is not None or l2 != '':
                initial = tf.truncated_normal(shape, stddev = 1)
                weight_decay = tf.multiply(tf.nn.l2_loss(initial), l2, name='weight_loss')
                tf.add_to_collection('losses', weight_decay)
                return tf.Variable(initial, dtype = tf.float32)
            else:
                return

        # 每次训练数据，越大则消耗越多,设置为动态参数
        batch_size = tf.placeholder(tf.int32,[])
        
        # 初始化俩层的权重W和偏置项b。我们之前标准化按照均值为0，标准差为1的方式，所以此处也如此。根据矩阵表达式，bias值为W的列。
        W = {
              'in' : weight_variable([FLAGS.feature_col,FLAGS.cell_num],FLAGS.l2_normal), 
              'out' : tf.Variable(tf.truncated_normal([FLAGS.cell_num,FLAGS.output_num],stddev = 1),dtype = tf.float32)
            }
        
        bias = {
                 'in' : tf.Variable(tf.constant(0.1,shape = [FLAGS.cell_num,]),dtype = tf.float32),
                 'out' : tf.Variable(tf.constant(0.1,shape = [FLAGS.output_num,]),dtype = tf.float32)
               }
        
        # 设定防止过拟合的开关,也给其一个动态参数
        keep_prob = tf.placeholder(tf.float32,[])
        learning_r = tf.placeholder(tf.float32,[])
        
        # 输入数据的格式,第一个为数量,因为我将timestep跟特征拼接了,所以是二维空间
        # 初始化标准化的GAMA和beta和均值，方差(我们这里设置为均值为0，方差为1的正态分布数据)
        input_x = tf.placeholder(tf.float32,[None,FLAGS.time_train_step * FLAGS.feature_col])
        input_y = tf.placeholder(tf.float32,[None,FLAGS.output_num])
        gamma = tf.Variable(tf.ones([FLAGS.time_train_step * FLAGS.feature_col]))
        beta = tf.Variable(tf.zeros([FLAGS.time_train_step * FLAGS.feature_col]))
        lstm_mean = tf.Variable(tf.zeros([FLAGS.time_train_step * FLAGS.feature_col]), trainable = False)
        lstm_variance = tf.Variable(tf.ones([FLAGS.time_train_step * FLAGS.feature_col]), trainable = False)
        
        
        # BN算法，batch_normal，批量正则化函数,然后再进入relu层和dropout层.input_x是维度,后面的均值，方差,beta,gama维度必须跟input_x相同即可
        input_x_normal = tf.nn.batch_normalization(input_x, lstm_mean, lstm_variance, beta, gamma, FLAGS.normal_epsilon)
        
        #通过reshape将输入的input_x转化成2维，-1表示函数自己判断该是多少行，列必须是feature_col
        input_x_r = tf.reshape(input_x_normal,[-1,FLAGS.feature_col])
        
      
        
        
        # relu_layer : input * W + b 
        input_rnn = tf.nn.dropout(tf.nn.relu_layer(input_x_r, W['in'], bias['in']), keep_prob)
        
        # 即LSTM接受：batch_size个，time_train_step行cell_num(神经元个数)列的矩阵
        input_rnn = tf.reshape(input_rnn, [-1, FLAGS.time_train_step, FLAGS.cell_num])
        
        # 定义一个带着“开关”的LSTM单层，一般管它叫细胞
        # 用softsign比tanh效果更好，更不容易饱和。具体看：https://blog.csdn.net/ningyanggege/article/details/80665888
        def lstm_cell(cell_num,keep_prob):
            cell = rnn.LSTMCell(cell_num, activation = tf.nn.softsign, reuse = tf.get_variable_scope().reuse)
            return rnn.DropoutWrapper(cell, output_keep_prob = keep_prob)
        
        # state_is_tuple：如果为True，则接受和返回的状态是n元组，其中 n = len(cells)。如果为False，则状态全部沿列轴连接。
        lstm_layers = tf.contrib.rnn.MultiRNNCell([lstm_cell(FLAGS.cell_num,keep_prob) for _ in range(FLAGS.layer_num)], state_is_tuple = True)
        
        # 初始化LSTM网络,以下是固定格式
        init_state = lstm_layers.zero_state(batch_size, dtype = tf.float32)
        
        # 组合LSTM神经网络
        outputs, state = tf.nn.dynamic_rnn(lstm_layers, inputs = input_rnn, initial_state = init_state, time_major = False)
        
        # 简单说上面的state是输出一种格式，或者shape，下面h_state是输出层的输入数据格式。
        h_state = state[-1][1]
        
        
        # 将LSTM层的输出，输入到输出层. out为我们模型预测出的结果
        # out = tf.matmul(h_state, W['out']) + bias['out']
        y_pre = tf.matmul(h_state, W['out']) + bias['out']
        
        # 用softmax进行回归分类 ,y_pre为预测出的结果 
        # y_pre = tf.nn.softmax(out,name = 'y_pre')  
        
        # 创建损失函数 , 记住要用真实的跟预测的out相比。
        # 以后版本用tf.nn.softmax_cross_entropy_with_logits_v2
        
        # loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = out, labels = input_y))

        # 下面这个是线性的方式，需要取消softmax步骤
        loss_metal = tf.reduce_mean(tf.square(tf.subtract(y_pre, input_y)))
        
        # 下面是将变量名加载道当前图中。我们只取loss
        tf.add_to_collection('losses', loss_metal)
        loss = tf.add_n(tf.get_collection('losses'))
        
        # 初始化优化器
        opt = tf.train.AdamOptimizer(learning_rate = learning_r)
        train_op = opt.minimize(loss)     
        
        # 不是回归，所以我们用分类的方式表示损失，如下所示，越高越好。
        # correct_pred = tf.equal(tf.argmax(y_pre, 1), tf.argmax(input_y, 1))
        # accurary = tf.reduce_mean(tf.cast(correct_pred, tf.float32))       
        
        init = tf.global_variables_initializer()
    
        # 用以保存参数的函数（跑完下次再跑，就可以直接读取上次跑的结果而不必重头开始）
        ckpt_path = os.path.join(FLAGS.checkpointDir, FLAGS.model_out_path)
        saver = tf.train.Saver()
        
        
    # 初始化GPU环境 
    config = tf.ConfigProto()
    config.gpu_options.allow_growth = True
    lr = FLAGS.learning_rate
    #使用with，保证执行完后正常关闭tf
    with tf.Session(config = config,graph = g1) as sess:     
        # 先判定模型保存路径是否有数据，有模型则使用模型的参数
 
        # module_file = tf.train.latest_checkpoint(FLAGS.model_in_path)
        module_file = os.path.join(FLAGS.checkpointDir, FLAGS.model_out_path)
        saver.restore(sess, module_file)
        print ("成功加载模型参数")     
                # 开始预测数据
        print('模型开始预测...')
        batch_size_test = FLAGS.batch_size 
        first = 0
        
        # 初始化测试集存储空间
        # label的初始化变量
        y_pre_test = np.zeros(shape = (batch_size_test,FLAGS.output_num))
        y_pre_test = np.delete(y_pre_test,[i for i in range(batch_size_test)],axis = 0)
        # ID的初始化变量
        test_id = np.zeros(shape = (batch_size_test,FLAGS.data_id))
        test_id = np.delete(test_id,[j for j in range(batch_size_test)],axis = 0)
        # 测试集结果数 = ID + LABEL (因为我们不需要feature)
        test_result = np.zeros(shape = (batch_size_test,FLAGS.output_num + FLAGS.data_id))
        test_result = np.delete(test_result,[z for z in range(batch_size_test)], axis = 0)
        num_test_range = (FLAGS.test_num // batch_size_test)
        # print(num_test_range)
        # 下面开始加载测试数据，用模型参数去跑测试集，并拼接成结果numpy数组
        for i in range(num_test_range):
            data_test, test_x, test_y = get_test_data(data_test = test,batch_size_first = first,batch_size_end = batch_size_test, test_path = FLAGS.test_path)

            temp = np.append(data_test,sess.run(y_pre, feed_dict={input_x: test_x, keep_prob: FLAGS.keep_prob_end,batch_size:test_y.shape[0]}),axis = 1)
            # 存放到总表里
            test_result = np.append(test_result,temp,axis = 0)
            print(test_result.shape)
            first += FLAGS.batch_size 
            batch_size_test += FLAGS.batch_size
            
        print(y_pre_test.shape)
        print(test.shape)
        print(test_result.shape)
        
        str_result = ''
        list_temp = []
        # 防止同步到ODPS时出现超过256的情况，这里保留1位小数。
        for i in range(len(test_result)):
            for j in test_result[i].tolist():
                list_temp.append(round(j) if type(j) == float else j)
            str_result += str(list_temp) + '\n'
            list_temp = []
        
        # 保存模型结果文件
        tf.gfile.FastGFile(FLAGS.data_result + 'submission.txt', 'w').write(str_result)       
      
        print('模型预测全部结束')



def main(_):
    train()
    
# 设置参数时,不一定所有参数都用ArgumentParser设置，比如batch_size和保留率我们会在循环中发生改变，就用tensorflow方式设置比较好。下面
# 都是比较固定的参数我们会设置动态参数。以后神经网络多时还要修改。
if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--buckets', type = str, default = '', help = 'bueckts桶')
    parser.add_argument('--checkpointDir', type = str, default = '',help = '模型保存路径的默认路径')
    parser.add_argument('--max_steps', type = int, default = 10000, help = '训练集最大迭代次数')
    parser.add_argument('--learning_rate', type = float, default = 0.01, help = '学习率' )
    parser.add_argument('--learning_rate_dec',type = float, default = 0.5, help = '学习下降率' )
    parser.add_argument('--keep_prob', type = float, default = 0.5, help = 'DropOut训练时最大保留率')
    parser.add_argument('--keep_prob_end', type = float, default = 0.5, help = 'DropOut模型保存时最大保留率')
    parser.add_argument('--time_train_step', type = int, default = 90, help = '训练集的最大天数')
    parser.add_argument('--time_label_step', type = int, default = 1, help = '预测最大天数')
    parser.add_argument('--feature_col_label', type = int, default = 3, help = 'label的feature维度,默认为1')
    parser.add_argument('--feature_col', type = int, default = 3, help = 'feature维度')
    parser.add_argument('--layer_num', type = int, default = 3, help = '网络层数')
    parser.add_argument('--cell_num', type = int, default = 720, help = '第一层隐藏层的神经元个数')
    parser.add_argument('--output_num', type = int, default = 3, help = 'label维度')
    parser.add_argument('--data_id', type = int, default = 4, help = 'id的维度,这里是客户+商户+业态+用户ID=4')
    parser.add_argument('--l2_normal', type = float, default = 0.01, help = 'L2正则项')
    parser.add_argument('--max_num', type = float, default = 10, help = '最大迭代次数')
    parser.add_argument('--batch_size',type = int, default = 200, help = '每次迭代数量')
    parser.add_argument('--loss_min',type = float, default = 0.1, help = '训练集和验证集的最小损失值')
    parser.add_argument('--normal_epsilon',type = float, default = 0.001, help = '标准化中防止除以0的情况所留的小数')
    parser.add_argument('--is_training', type = bool, default = True, help = '读取数据是否为训练集，默认为True,False为验证集')
    parser.add_argument('--is_time_filt', type = int, default = 6, help = '默认过滤时间的大小，目前设置为6，以后可能改成8')
    parser.add_argument('--train_path', type = str, 
                        default = 'train*',
                        help = '训练集输入路径')
    parser.add_argument('--test_path',type = str,
                        default = 'test*',
                        help = '测试集输入路径')
    parser.add_argument('--model_out_path', type = str, 
                        default = 'tensorflow_model',
                        help = '模型输出路径')
    parser.add_argument('--model_in_path', type = str, 
                        default = 'tensorflow_model',
                        help = '模型输入路径')
    parser.add_argument('--total_num', type = int, default = 20000, help = '总数据输入数量')
    parser.add_argument('--train_num', type = int, default = 15000, help = '训练集输入数量')
    parser.add_argument('--valid_num', type = int, default = 5000, help = '验证集输入数量')
    parser.add_argument('--test_num', type = int, default = 2000, help = '模型输出数量')
    parser.add_argument('--min_num', type = int, default = 0, help = '最小数值')
    parser.add_argument('--data_result',type = str,
                        default = 'oss://bucketjrm2.oss-cn-shanghai-internal.aliyuncs.com/data_result/',
                        help = '模型结果输出路径')



    FLAGS, _ = parser.parse_known_args()
    tf.app.run(main=main)
