https://mp.weixin.qq.com/s/Npy1-zrRmqETN8GydnIb8Q
https://blog.csdn.net/lc013/article/details/87898873
https://blog.csdn.net/joycewyj/article/details/51647036
https://blog.csdn.net/fisherming/article/details/79925574
https://blog.csdn.net/han_xiaoyang/article/details/50503115
https://blog.csdn.net/han_xiaoyang/article/details/50481967
https://blog.csdn.net/JoyceWYJ/article/details/51647036
https://blog.csdn.net/hongxingabc/article/details/74357178
https://blog.csdn.net/jiangjiang_jian/article/details/82081755


一. 算法相关技术
1.数据属性
(1)标称属性: 头发,颜色,或者人物的状态:已婚,程序员等。这类数据不是定量,无法用均值/中位数,用众数去中心趋势度量。
            PS:这类数据也可用众数补充缺失值(中心趋势度量往往可以用来补充缺失值).
(2) 均值,中位数,众数,中列数: 
    a.均值： 优点: 时间复杂度: O(n).计算速度快,sum(n+m)的栈方式计算。还可以根据加权取平均值(权重可以时出现比例，或者业务权重或者模型产生的权重)。
             缺点:对于异常/离群点数据很敏感，影响中心结果分布。
             解决方案: 根据离群点分左右对称倾斜,可以用3θ((x-avg)>3*std,满足这个公式定为离群点)/者截尾去均值法(前后2%左右)/Kmeans举例去离群点处理.
             解决方案带来的负面: 带来了时间消耗开支,具体需要根据情况酌情处理。
    
    b.中位数: 优点: 对于异常/离群点数受到的影响比较小。
              缺点：时间复杂度： n.log(n)+n/1 。 基本是比均值处理的N倍时间。因为排序目前最好的方法是归并+插入/快速+插入。但都无法走出n.log(n)的复杂度。
              解决方案:(1)在已知数据分布的情况下，分区计算，比如划分工资为1000-2000，2000-5000，50000-100000.。。。。 然后取中间的中位数即可。
                      (2) 插值计算求出中位数的近似值: median = (L1 + (N/2-(Σfreq))/freq(median))width
                          N是数量,L1是中位数区间下届, Σfreq是低于中位数区间的所有区间频率和,freq(median)是中位数区间的频率。width是中位数区间的宽度.
      
    c. 众数: 单峰(单个众数),双峰,多峰。
             部分业务场景需要的统计数据，并解决标称属性和部分业务的缺失特性.
             求众数近似值: 设众数为Mode,数据集为左或右倾斜:
             mean - mode ≈ 3 * (mean - median)
             
             
    d. 中列数: (min+max)/2,对称分布的均值的粗略估计。
             
    PS:时间复杂度为O(n)的求众数代码如下(这里定义的众数是超过一半的数),时间复杂度为O(n)
    java/c++版本(非指针): 
    int temp;
    int num = length(arr)
    for(int i =0,c = 0; i <num; i++ ){
     if(c==0){
       temp = arr[i];
       c++;
     }else{
       if(temp == arr[i])
         c++; 
       else
         c--;
     }
    }
    
    python版本:
    temp = arr[0]
    c = 0
    
    for i in arr:
        if c == 0:
            temp = arr[0]
            c += 1
        else:
            if temp == arr[i]:
                c += 1
            else:
                c -= 1
  (3) 二元属性: 就是bool类型, 0和1 
  (4) 序数属性: 描述类似于将连续型数值转为离散后的结果,比如某个区间用1表示，另一个区间用2表示，或者军人为1，士兵为2，学生为3.他们之间往往有某种关系。
               他们的中心趋势用众数/中位数(有序数列才能用中位数)表示，不能用均值。不能将不同类别的序数量化，比如1比0类型强多少。
               作用: (1) 将连续型数值分区间离散化,使其变得有序性。 0-20->低龄。 21-40-》中年龄....
                     (2) 主观测量,比如对问卷满意度调查评级为0：满意，1:一般满意...
               
  PS:以上三者类型数据都是定性，只是描述数据的特征,但无法量化(可测量),通常用数值型来量化(可测量)。
  (5) 数值属性: a.区间数据: 就是数字,但不能比较,比如10度比5度温暖2倍是不对。
                b.比例数据: 这种可以比较。重量,速度等。
  (6) 离散和连续:连续=数值，离散就是其他。
  (7) 中心趋势度量: 一组数据中大部分数据落在哪里。
  PS: 正倾斜： 众数小于中位数的数列。
      负倾斜:   众数大于中位数的数列。
             
    
2.数据处理
A.度量数据散布：极差，四分位数，方差，标准差，四分位数极差。
(1) 极差: 最大值 - 最小值。

(2) 分位数，2分位数和四分位数最常用。
四分位极差： IQR = Q3 - Q1。 这个是第三分位数-第一分位数。
Q1,Q2,Q3,Q4都是各自区间的中位数，Q越大，说明数据波动越大。
所以四分位极差的意义是越大说明数据波动越大。
其次，可以用Q1和Q3来挑离群点。Q3之上的1.5*IQR处的值以上或者Q1以下的1.5*IQR的值以下为
数学界默认的离群点，具体也根据数据分布来判断。

(3) 五数概括： 用5个数来概括数据分布。
最小值，Q1,中位数，Q3,最大值。 差距越大说明数据越不稳定。

(4) 盒图。体现了五数的概括。  这个不太懂，项目时在使用。
盒的端点在四分位数上。使得盒的长度是四分位数极差IQR.
中位数用盒内线标记。
盒外线延伸到最小和最大观测值。
如果观测值的最大值和最低值不到1.5 * IQR时，需要扩展他们。否贼在此终止。

(5) 方差和标准差
标准差关于均值的发散，仅当我们使用均值为中心度量时才使用。`
(1 - 1/k^2) * 100%的观测均值不超过k个标准差。 这句话你可以理解为 (x - avg(x))<=3*std(x) 也就是在均值为中心趋于分布时，用标准差来衡量波动性。

(6) 常用的距离公式与他们的应用意义
 距离的意义是在处理数据特征时,往往要进行相似性度量的考察,以及很多指标（推荐类）也需要进行一些相似性的计算，这时候都需要距离。
 a.欧式距离:   直接的距离差值和开方再开根号。表示真实的距离。
   曼哈顿S:    @1: 俩个点:开根号(x1-y1)^2 + (x2-y2)^2
   曼哈顿S=|X1-X2| + |Y1-Y2|，表示坐标距离，一般很少用，但如果你要求类似路程，或者家门口距离轻轨的真实距离，你就要用曼哈顿，此时欧式距离就是所谓的直线距离，并非人真实走的距离。
   闵可夫司基: 各个维度坐标差值开N次方的和再开N次方的根号。
              @2: 俩个N维: 开根号Σi=1(Xk1-Xj1)^2 或向量表示: 开根号(a-b).(a-b)^T
              优点: 体现绝对值的差异,跟方向无关系。比如我们分析用户活跃度,登陆次数,平均观看时用SQL规则/Kmeans聚类时就需要用欧式距离。
                    因为这类值往往是(1,10)和(10,100)的比较，此时相似度很高，用余弦不好，而该用绝对值的欧式距离,这样它们之间的差值很大，可以分析出。
              三者距离公式的区别:
                    (1) 由于导数原因,范数为1的曼哈顿在二次导数不变,一次导数比值下降,提前为0，会导致很多0的情况,更适合用特征的选择.欧式反之。
                    (2) 曼哈顿比较的是你的路程距离，而不是最近距离。欧式距离比较的是你的真实距离，也就是最短距离，有些业务需求比如车程，路程就得
                        用曼哈顿。
                        
                    
              缺点: 它将样本的不同属性（即各指标或各变量量纲）之间的差别等同看待，这一点有时不能满足实际要求。
                   比如年龄和学历对工资的影响，将年龄和学历同等看待；收入单位为元和收入单位为万元同等看待。
              改进方案: (1) 对于量纲不同的数据集，我们进行标准化处理，区间设置在[0,1]之间，减少量纲的影响。
                       (2) 对于数据带有波动时,对坐标加权，使变化较大的坐标比变化较小的坐标有较小的权系数。
              但是: 无论是加权(针对数据波动或业务需求)还是标准/归一化,我们都没考虑到总体变异对距离远近的影响,这点目前我是没想出解决方法。
              PS:总体变异性指个别差异数据导致整体数据产生了部分变异。
  b.切比雪夫距离: dis = MAX(|x1 - x2| + |y1 - y2|).  他也是闵可夫斯基距离的最大维度。
                 这个在坐标轴上与曼哈顿是轴旋转45°的，你可以理解为最大距离的取值，有时候衡量最坏情况会用，另外时间复杂度高，O(n)
  c.马氏距离: 协方差矩阵,表示数据的相似度而不是距离远近。
             公式: DM(x)=开根号(x−μ)T∑−1(x−μ)
                   如果∑−1∑−1是单位阵的时候，马氏距离简化为欧氏距离。 

             优点: 马氏距离不受量纲的影响，两点之间的马氏距离与原始数据的测量单位无关；由标准化数据和中心化数据(即原始数据与均值之差）计算出的二点之间的马氏距离相同。
                   马氏距离还可以排除变量之间的相关性的干扰。 
             缺点: 夸大了变化微小的变量的作用。
      
   d.标准化欧式距离: 既然数据各维分量的分布不一样，那先将各个分量都“标准化”到均值、方差相等。
                    公式: dis = 开根号Σ(Xk1-Xk2/std(Xk))^2. 标准化是将结果化为期望为0，方差为1.你可以当作1/std(Xk)为权重,这也是加权欧式距离。
                   
   e.巴氏距离: 概要: 衡量分类数据中数据与数据的可分离性,通过概率统计两个统计样本之间的重叠量的近似测量，可以被用于确定被考虑的两个样本的相对接近。
              
              公式: 离散型数据: Dis(p,q) = -ln(开根号ΣP(px).P(qx))  P为概率。
                   连续型数据: ∫开根号P(px).P(qx).dx
                             
              优点与机器学习应用: 采用巴氏距离特征选择的迭代算法，可以获得最小错误率上界。当特征维数高时，为了减少巴氏距离特征选择计算时间
              ，对样本先进行K-L变换，将特征降低到中间维数。然后进行巴氏距离特征选择，降低到结果的维数。
              用基于MNIST手写体数字库的试验表明，该文方法比单纯用巴氏距离特征选择计算时间大大减少，
              并比主分量方法(即单纯使用K-L变换)特征选择的错误率小得多.
              
              PS:目前机器学习的传统特征选择方法有PCA和巴氏距离选择法,PCA主要保留的是描述样本分布的特征,并非最有利于
              分类的特征，但巴氏距离给出样本的正态分布下的Bayes最小错误率上界,与错误率挂钩，理论上能够取得相当好的
              分类特征，但难以获得分析解。所以,先将主成分降低空间维度,然后使用迭代利用巴氏距离与错误率上界的
              关系,在相对低纬度空间求解特征选择矩阵。-- 在高纬度中比较有效。
              
   f.汉明距离: 两个等长字符串s1与s2之间的汉明距离定义为将其中一个变为另外一个所需要作的最小替换次数。例如字符串“1111”与“1001”之间的汉明距离为2。
              应用：信息编码（为了增强容错性，应使得编码间的最小汉明距离尽可能大）。
              
   g.夹角余弦Consine: 几何中夹角余弦可用来衡量两个向量方向的差异，机器学习中借用这一概念来衡量样本向量之间的差异。
              公式:cosθ = x1.x2+y1.y2 / 开根号(x1^2+y1^2).开根号(x2^2+y2^2)
              范围[-1,1]
   h.杰卡德相似系数: |A∩B|/|A∪B| 。关联分析指标中用过。
     杰卡德距离: Dis(A,B) = |AUB| - |A∩B|/|A∪B|
     杰卡德距离用两个集合中不同元素占所有元素的比例来衡量两个集合的区分度。
     作用:衡量样本的相似度
   
   g.皮尔逊系数： 
     相关系数: Cov(X,Y)/开根号D(X).开根号D(Y) = E((X-EX).Y(Y-EY))/开根号D(X).开根号D(Y)
              E期望或均值，D方差。
     相关系数衡量随机变量X与Y相关程度的一种方法，相关系数的取值范围是[-1,1]。相关系数的绝对值越大，
     则表明X与Y相关度越高。当X与Y线性相关时，相关系数取值为1（正线性相关）或-1（负线性相关）。
     定义: 皮尔逊相关系数定义为两个变量之间的协方差和标准差的商
     应用场景的数据:当两个变量的标准差都不为零时，相关系数才有定义，皮尔逊相关系数适用于：
                   (1)两个变量之间是线性关系，都是连续数据。
                   (2)两个变量的总体是正态分布，或接近正态的单峰分布。
                   (3)两个变量的观测值是成对的，每对观测值之间相互独立。 
     用来检验用来检验样本一致性。
     缺点：必须符合正态分布。
     
  各种距离总结:
  总结
  简单说来，各种“距离”的应用场景简单概括为：
    空间：欧氏距离
    路径：曼哈顿距离
    国际象棋国王：切比雪夫距离
    以上三种的统一形式:闵可夫斯基距离

    加权：标准化欧氏距离
    排除量纲和依存：马氏距离
    向量差距：夹角余弦
    编码差别：汉明距离
    集合近似度：杰卡德类似系数与距离
    相关：相关系数与相关距离。
    
    PS:三大用来计算相似性的公式对比分析:
    a.余弦/皮尔逊距离: 主要体现方向的差异。
    b.杰卡德: 主要体现非对称的出现次数的差异，但因为不能考虑样本的特征性，方向差异，数值差异等样本本身的问题，所以受到局限性。
              不过可以当作一种相似性的特征来融入样本中，比如作为关联分析的一种评估，说明了商品之间的出现个数的相似性作为一个特征后续加权处理。
    c.调整余弦相似(推荐中常用):
      因为余弦/皮尔逊更多是方向的差异但不是数值量纲的差异,比如在某评分业务上用户A,B对X,Y的评分(1,2),(4,5) 中余弦/皮尔逊相似度为0.98.
      但其实用户不喜欢这俩个内容，都很低，所以无法说明用户A,B相似。此时我们评分都减去将X,Y的均值3。（-2，-1）和（1,2），再用余弦相似度计算，得到-0.8，
      相似度为负值并且差异不小，但显然更加符合现实。如果我们还知道用户的行为，标签，那么再减去这些属性的数值，将会更加准确。所以相似度的计算
      是需要我们做数据处理应用的。
      
(7) 缺失值处理:
    (1)忽略元组: 只用于分类任务的数据。
       当一个数据是一组时，比如 人的属性这个特征的值: (17,2000,35) 这种，可以使用忽略元组的方式。
       也就是一旦这组数据一个地方缺失，剩下属性也当缺失值处理。
       
       (2)使用全局常量:
       比如英文的UnKown 或者中文的 无。
       
       (3)使用中心度量。
       a. 如果是连续波动性不大数据或者对称的数据 --> 均值。
       b. 如果数据有比较大的倾斜，波动性强。 --> 中位数。
       下面是我个人认为，需要在项目中实验:
       c. 如果数据像某一边极度倾斜，用4分位根据情况的中位数取代。
       d. 如果数据属于少数重复很多的数组，用众数取代，也就是频率最高的数。
       e. 某系用户的常见行为,比如大家通常都在周五买菜，这种也是众数取代。
       
       (4) 将同类型客户的数据分布的均值或者中位数取代缺失值客户的值。
       比如: 我们的b客户有缺失值，那么将与b客户相似度最高的a客户的缺失值分布如果是波动小或者对称就取均值，然后将其填满取代。
       
       (5)用线性回归，决策树，贝叶斯等机器学习算法去将非缺失值其他用户数据去拟合得到一个更好的数，给填补，比如构建决策树等，这个方法要更好，
       但需要一些场景，需要我后期实验。而随着集成学习的出现，用随机森林，GBDT要更好。
       比如：将数据表中某些没有缺失的特征属性来预测某特征属性的缺失值。但这个的前提是其他变量必须与缺失值变量有关。
       
       (6) 根据数学的差值法和滑动平滑差值法
       
       a.滑动平滑差值法：
       一个列表a 中的第 i 个位置数据为缺失数据，则取前后 window 个数据的平均值，作为插补数据。
       举例: a = [3,4,5,6,None,4,5,2,5] 、 window = 3 
       则 None位置的数据为：(4+5+6+4+5+2)/6 = 2.67
       
       b. 拉格朗日差值法:
       具体看：https://blog.csdn.net/zhangyue_lala/article/details/64437547
       在sk-learn中是data.interpplate.
       即构造一个多项式，Li（x），让n=i的时候Li（x）=1，当n≠i时候Li（x）=0，这样就保证了Ln（x）通过每一个（xi，yi）点，符合插值原理。
       Li(x) = (x-x0)...x(x-xn) / (xi-x0)...(xi-xn)
       这个就是插值多项式系数，它保证了Li(xi)=1，而带入其他点都为0，yi*Li（xi）就得到插值多项式的每一项，这个多项式通过每一个已知点。
       (1, 2618.2), (2, 2608.4),(3, 2651.9),(4, 3442.1), (5, 3393.1)
       PS:L(x) = -94.97 x^4 + 1065 x^3 - 3991 x^2 + 5930 x^1 - 291.4 
          把 x= 0 带入得到 L(x)，就得到了 -291.4。这里x=0就是L(x)的截距。直观感觉就不太合理，
          猜测就是拉格朗日插值法对边缘数据敏感（即插值需要左右两边数据提供信息，在缺失左边数据信息情况下，得到的结果就不太合理），日后求证！
          也就是i = 0 ，第一个数为缺失时，此方法太敏感。


       缺点：
       1.运算量很大
       2.稳定性不好，即Ln（x）的却能通过所有已知点，但是，不能保证Ln（x）在已知点附近的值是否有意义，可能它的波动会大到超出实际意义。

      
       拉格朗日差值法缺点:猜想: 拉格朗日插值法对边缘数据敏感（即插值需要左右两边数据提供信息，在缺失左边数据信息情况下，得到的结果就不太合理）
       比如上面中第一个数3，最后一个数5就是边缘数据，但上面数据是都存在的。
       
       (7)其他方法：上下数，0，
       
       (8) 映射法： 数据量大，机器成本高时用这个。
       映射法，也就是变量映射到高维空间。但需要很高的成本。
       比如： 性别中：男，女，缺失 -》映射成: 是否有男，是否有女，是否缺失。
       连续值也是这样处理，但能达到几亿维度。 最后会是一个很稀疏的方法，再用FFM,嵌入等方式处理。
       
       (9) 如果缺失值太大了，就删除吧。
       
       (10) 特殊值标记
       引入虚拟变量来表特征是否有缺失，是否有补全。
       
 (8) 噪点处理:
     做噪点数据(离群点时),要根据业务情况，不一定要清除掉/或用其他数据取代,有时候某些业务场景噪点是复合的,做完之后要根据业务去确认。
     比如某超市做活动导致GMV短暂提升，就不是噪点原因。
     
     a.分箱法(也可作为离散。)
     有俩种方式:
     a.平均分箱法:
     将数据分成3类，然后每类都变成一个数。这个数可能是均值或者中位数。
     --a.24,25,26
     --b.70,78,80
     --c. 100110,120  =>
     --a. 25,25,25
     --b. 76.76.76
     --c . 110.110.110
     
     b.边界分箱法:
     同理还是上面数据->
     24,24,25
     70,70,90
     100,100,120
     也就是取俩边数，然后根据情况将最小或最大的一直重复写，另外一个就当做边界写一个。
     
     (2)回归法
     用线性回归平滑数据。
     
     (3) 聚类离群点分析
     Kmeans找出最大簇心距离。
     
     (4)冗余和相关分析
     PS:标称型：一般在有限的数据中取，而且只存在‘是’和‘否’两种不同的结果（一般用于分类）
        数值型：可以在无限的数据中取，而且数值比较具体化，例如4.02,6.23这种值（一般用于回归分析
     
     一个属性被另一组属性导入或者导出，这个属性可能是冗余的。
     a. 标称数据： 卡方检验。--》用于相关分析比较多，本质是比较真实出现频数与目标出现频数的差距。
        一般是假设了多个特征，然后验证这些特征对结果label产生的相关性。如果结果越小，越不相关，越独立。
     先看这个: https://blog.csdn.net/snowdroptulip/article/details/78770088
     说明: 主要是比较两个及两个以上样本率( 构成比）以及两个分类变量的关联性分析。其根本思想就是在于比较理论频数和实际频数的吻合程度或拟合优度问题。
     公式: 卡方 = Σ(A-T)^2 / T  A为实际值,T为理论值。值越大,说明越相关，越不独立。
     举例:
     我们想知道不吃晚饭对体重下降有没有影响：



 	体重下降	体重未下降	合计	体重下降率
            吃晚饭组	123	467	590	20.85%
            不吃晚饭组	45	106	151	29.80%
            合计	168	573	741	22.67%
            
            1. 建立假设检验：
            
            H0：r1＝r2，不吃晚饭对体重下降没有影响，即吃不吃晚饭的体重下降率相等；
            H1：r1≠r2，不吃晚饭对体重下降有显著影响，即吃不吃晚饭的体重下降率不相等。α=0.05
            
            2. 计算理论值
            我们先假设是没影响,也就是完全独立。所以吃完饭组=590*(独立的体重下降率)= 590*22.67% = 133.765. 其他依次类推，如下：
             	体重下降	体重未下降	合计
            吃晚饭组	133.765	456.234	590
            不吃晚饭组	34.2348	116.765	151
            合计	168	573	741
            
            3. 计算卡方值
            根据公式=》(123-133.765)^2/133.765+(467-456.234)^2/456.234+(34.2348-45)^2/34.2348+(116.765-151)^2/116.765 = 5.498
            
            4. 查卡方表求P值
            
            在查表之前应知本题自由度。按卡方检验的自由度v=（行数-1）（列数-1），则该题的自由度v=（2-1）（2-1）=1，
            查卡方界值表，找到3.84，而本题卡方=5.498即卡方＞3.84，P＜0.05，差异有显著统计学意义，说明95%的概率，不吃晚饭对体重下降是相关的。
            按α=0.05水准，拒绝H0，可以认为两组的体重下降率有明显差别。
            通过实例计算，对卡方的基本公式有如下理解：若各理论数与相应实际数相差越小，卡方值越小；如两者相同，则卡方值必为零。
            
            简单说就是：假设不想关，公式得到答案看是否大于卡方列表的数据（一般是95%的概率成立，即0.05）.如果大于卡方列表的值，说明相关，小于说明不相关。
            
            
            
            
            
            
            通过实例计算，对卡方的基本公式有如下理解：若各理论数与相应实际数相差越小，卡方值越小；如两者相同，则卡方值必为零。

           再看这个: https://blog.csdn.net/u014755493/article/details/69791657
           卡方的应用:

            (1)检验某个连续变量的分布是否与某种理论分布相一致。如是否符合正态分布、是否服从均匀分布等
            
            ①提出原假设H0：变量符合F(x)分布(针对连续型变量)，若针对离散型变量，则要假设其分布律
            
            ②将样本划分区间k个，每个区间样本数不小于5，区间互不相交，获得每个区间的实际频数fi
            
            ③根据假设分布的分布函数，求出每个区间的理论概率pi，得到理论频数npi（n为样本总数）
            
            ④计算卡方统计量 
            
            ⑤计算自由度，即区间数减1，假设显著性α=0.05，得到x2(k-1)α临界值，如果卡方统计量大于临界值，说明理论与实际偏差过大，拒绝原假设
            
            
            
            (2)检验某个分类变量各类的出现概率是否等于指定概率。如在36选7的彩票抽奖中，每个数字出现的概率是否各为1／36；掷硬币时，正反两面出现的概率是否均为0．5。
            
            ①提出原假设H0：假设该各类变量符合出现概率
            
            ②根据原假设得出理论频数，即对各分类变量其对应概率为pi，则理论频数为npi（n为样本总数）
            
            ③根据已有实际观测值fi，计算卡方统计量即 
            
            ④计算自由度，为分类变量数目减去一，与再显著性α=0.05下的临界值比较，若大于临界值，则认为偏差过大，拒绝原假设
            
            
            
            (3)检验某两个分类变量是否相互独立。如吸烟(二分类变量：是、否)是否与呼吸道疾病(二分类变量：是、否)有关；产品原料种类(多分类变量)是否与产品合格(二分类变量)有关。该问题针对列联表。
            
            ①提出原假设H0：两个分类变量之间无关
            
            ②再假设无关的条件下，应用其独立同分布特点，计算出每个格子的理论概率值，比如吸烟并且没有呼吸道疾病的概率值，利用样本数据，分别求出吸烟的概率和患呼吸道疾病的概率，两者相乘得到联合概率，再乘以样本总调查数，就得到了理论数，这里有一个前提很重要，就是我们假设了分类变量之间独立，再能将其概率相乘。
            
            ③根据样本，得到实际观测值，计算出卡方统计量
            
            ④列联表自由度为（列数-1）（行数-1），再与显著性α=0.05下的临界值比较，若大于，则拒绝原假设，认为有关
            
            
            
            (4)检验控制某种或某几种分类因素的作用以后，另两个分类变量是否相互独立。如在上例中，控制性别、年龄因素影响以后，吸烟是否和呼吸道疾病有关；控制产品加工工艺的影响后，产品原料类别是否与产品合格有关。
            
            (5)检验某两种方法的结果是否一致。如采用两种诊断方法对同一批人进行诊断，其诊断结果是否一致；采用两种方法对客户进行价值类别预测，预测结果是否一致。
            
            第四和第五种方法同上，关键都在于求出理论频数，这是构造卡方统计量的关键。

     
     
     b. 数值型： 共线性处理
     相关系数--》corr检验/协方差。+ 容忍度
     具体如下：
      (1) 导致多重共线性的原因:
          a. 解释变量都共享有共同的时间趋势.
          b. 一个解释变量是另一个的滞后(落在形式的发展后面)，二者往往遵循一个趋势；
          c. 由于数据收集的基础不够宽，某些解释变量可能会一起变动；
          d. 某些解释变量间存在某种近似的线性关系；
         
      (2) 具体策略：
          a.相关性分析，相关系数高于0.8，表明存在多重共线性；但相关系数低，并不能表示不存在多重共线性；
          b.容忍度（tolerance）与方差扩大因子（VIF）。某个自变量的容忍度等于1减去该自变量为因变量而其他自变量为预测变量时所得到的线性回归
            模型的判定系数。容忍度越小，多重共线性越严重。通常认为容忍度小于0.1时，存在严重的多重共线性。方差扩大因子等于容忍度的倒数。
            显然，VIF越大，多重共线性越严重。一般认为VIF大于10时，存在严重的多重共线性。
          c.回归系数的正负号与预期的相反。
          
      (3) 解决方法:
          a. 增加样本容量：多重共线性问题的实质是样本信息的不充分而导致模型参数的不能精确估计，因此追加样本信息是解决该问题的一条有效途径。
          b. 如果要在模型中保留所有的自变量，那就应该：避免根据t统计量对单个参数β进行检验；对因变量y值的推断限定在自变量样本值的范围内。
          c. 删除一个或几个共线变量：实际操作中常用逐步法作为自变量筛选方法。
          d. 岭回归法（Ridge）；岭回归法是通过最小二乘法的改进允许回归系数的有偏估计量存在而补救多重共线性的方法
          e. 主成分分析法。
          
      (4) 处理原则:
          a. 多重共线性是普遍存在的，轻微的多重共线性问题可不采取措施；
          b. 严重的多重共线性问题，一般可根据经验或通过分析回归结果发现。如影响系数符号，重要的解释变量t值很低。要根据不同情况采取必要措施。
          c. 如果模型仅用于预测，则只要拟合程度好，可不处理多重共线性问题，存在多重共线性的模型用于预测时，往往不影响预测结果。
      
      (5) 相关系数+容忍度处理步骤:
          X.corr():
             A       B       C
          A:1.00     0.94    0.74
          B:0.94     1.00    0.82
          C:0.74     0.82    1.00
          A与B:0.94 > 0.8存在强相关。B与C也同理。
          所以针对AB,BC用方差扩大因子（VIF）检验：
          vif(X,digits=3(特征数))
          A       B       C
          3.19    8.85    12.29
          C大于10，存在严重共线性(VIF大于10)
          所以我们得知这三个特征是C导致了多重共线性,可以删除此特征。
          
       方差扩大因子计算公式:
       假若有X1 、X2 、X3三个自变量，X1的vif计算：
       1. x1对常数项、x2、x3回归，求出R^2
       2. VIF＝1/(1-R^2)
       3. X2 、X3的vif计算,以此类推。
       4. 不管几个x，都这样计算即可
          
     
     c.元组去重
     大概意思是需要一个对应的码值表去确定它的唯一性，如果类似同名购买不同商品其实是俩人这种，需要去重。
     
     d.数据集成--数值冲突的检测与整理
     对同一世界或实体，来自不同数据源的属性值可能不同，例如一个单位在中国这里是公里,另一个单位在英国是英里。一个大学的评分制度是A-F,另一个大学评分制度是1-10.或者数据库中不同库中不同表的属性值一样，但属性名不同。这些都是类似问题。
     这个问题看: https://baijiahao.baidu.com/s?id=1607427457685542773&wfr=spider&for=pc
     
     e.数据归约:
     (1)离散小波变换(图像处理) ==》 适合处理高纬数据
     数学和理论：https://www.cnblogs.com/jfdwd/p/9249850.html
     https://blog.csdn.net/u014485485/article/details/78672184
     
     实践: https://blog.csdn.net/zhuguorong11/article/details/70941901
   
   c.3θ法
     3∂原则: 这个原则有个条件：数据需要服从正态分布。在 3∂ 原则下，异常值如超过 3 倍标准差，那么可以将其视为异常值。
     正负3∂ 的概率是 99.7%，那么距离平均值 3∂ 
     之外的值出现的概率为P(|x-u| > 3∂) <= 0.003，属于极个别的小概率事件。
     PS: 如果数据不服从正态分布，也可以用远离平均值的多少倍标准差来描述。
     
   d.基于近邻度的离群点检测
    (1) 一个对象的离群点得分由到它的 k-最近邻（KNN）的距离给定。
    (2) 注意点: 这里需要注意 k 值的取值会影响离群点得分，如果 k 太小，则少量的邻近离群点可能会导致较低的离群点得分；
               如果 k 太大，则点数少于 k 的簇中所有的对象可能都成了离群点。为了增强鲁棒性，可以采用 k 个最近邻的平均距离。
    (3) 优缺点：
       a. 简单;
       b. 基于邻近度的方法需要 O(m^2) 时间，大数据集不适用；
       c. k 值的取值导致该方法对参数的选择也是敏感的；
       d. 不能处理具有不同密度区域的数据集，因为它使用全局阈值，不能考虑这种密度的变化。
       
   e.基于密度的离群点检测
     (1) 一种常用的定义密度的方法是，定义密度为到k个最近邻的平均距离的倒数。如果该距离小，则密度高，反之亦然。
     (2) 另一种密度定义是使用 DBSCAN 聚类算法使用的密度定义，即一个对象周围的密度等于该对象指定距离 d 内对象的个数。
     
   f.基于Kmeans离群点检测

 
     
 (9) 高纬映射（One-Hot）
     将属性映射到高维空间，采用独热码编码（one-hot）技术。
     将包含 K 个离散取值范围的属性值扩展为 K+1 个属性值，若该属性值缺失，则扩展后的第 K+1 个属性值置为 1。
     这种做法是最精确的做法，保留了所有的信息，也未添加任何额外信息，若预处理时把所有的变量都这样处理，会大大增加数据的维度。
     这样做的好处是完整保留了原始数据的全部信息、不用考虑缺失值；缺点是计算量大大提升，且只有在样本量非常大的时候效果才好。
     另外这种做法防过拟合能力很强。
     
 (10) PCA
 https://blog.csdn.net/program_developer/article/details/80632779
 
 (11) KL
 https://blog.csdn.net/jiangjiang_jian/article/details/82081755
 
    
  
           
            
3.特征工程:
(1)


4.关联分析
(1) Apriori算法
4种商品所有可能的组合。对给定的集合项集{0,3}，需要遍历每条记录并检查是否同时包含0和3,扫描完后除以记录总数即可得支持度。
对于包含N种物品的数据集共有2的N次方-1种项集组合，即使100种，也会有1.26×10的30次方种可能的项集组成。
出现次数的可能性: 2^N-1次方。 
优化: 为降低计算时间，可用Apriori原理：如果某个项集是频繁的，那么它的所有子集也是频繁的。
逆反：如果一个项集是非频繁集，那么它的所有超集也是非频繁的。

(2) FPGrowth算法
    a. 先扫描一遍数据集，得到频繁项为1的项目集，定义最小支持度（项目出现最少次数），删除那些小于最小支持度的项目，
       然后将原始数据集中的条目按项目集中降序进行排列。 
    b. 第二次扫描，创建项头表（从上往下降序），以及FP树。 
    c. 对于每个项目（可以按照从下往上的顺序）找到其条件模式基（CPB，conditional patten base）,递归调用树结构，删除小于最小支持度的项。
       如果最终呈现单一路径的树结构，则直接列举所有组合；非单一路径的则继续调用树结构，直到形成单一路径即可。


(3) 用处:关联分析: 这个算法本质是个基于频次的算法，也属于规则算法。重点在于我们的计算指标。

         1.支持度:
         
         (1)公式: surport(A) = P(A) = ITEM(A) / N 。
         
         (2)数学解释: ITEM是A的出现个数，N是总商品出现次数。
         
         (3)业务解释: 它表示某个商品，或者某个组合商品在一起的次数，比上总商品次数的比值。越高说明这个商品，或者这个组合商品越重要。
         
         (4)举例: 某个超市飞科剃须刀刷卖的好，它的支持度就大。飞科剃须刀跟香皂一起出现次数多，他们这个组合的支持度就大。
         
         2.置信度:
         
         (1)公式:confidence(A,B) = P(A U B) / P(A) = P(B|A)
         
         (2)数学解释: AUB 表示并集, 也就是A与B同时出现的概率。A是A单独出现的概率。
         
         (3) 业务解释: 商品A出现的概率，与商品A,B同时出现的概率的比值。
         这里的A不一定是一个商品，也可能是多个商品。
         记住，它仅仅表示先买B，再买A的概率。
         
         (3)举例: 先买啤酒，再买咸菜的概率很高，所以啤酒对咸菜的置信度很高。但
         先买咸菜再买啤酒的概率不高，说明咸菜对啤酒的置信度不高。
         
         
         3.提升度:
         
         (1)公式:lift(A,B) = P(A U B) / (P(A) * P(B)) = confidence(A,B) / P(B)
         
         (2)数学解释: A和B的提升度本质就是A和B的置信度/B的出现概率。
         
         (3)业务解释: 提升度出现的原因是因为置信度公式中有不足的地方。假如B出现的概率很高。A的概率不高，那么会导致买了B大部分都会买A。所以导致A,B同时购买的概率很高，但这不能说明A和B的关联性很强。因为这个关联性强的前提是B出现的概率太高了。所以提升度中为了避免这种问题，对B出现的概率进行了相除。
         这样假设confidence(A,B)很高是因为B的概率大，那么B越大，被除后结果越小，结果小说明A，B的相关性是因为B自身高而高，而实际相关性并不高。
         
         (4)举例说明: 某超市的某种品牌的水卖的很火，某人买了薯片再买这个水。因为这个薯片自身的销量并不太高，但因为水的销量很高。通过公式得到它俩同时出现的概率很大，所以置信度高，但再除以水的出现概率，这个结果值会降低，说明他俩同时出现概率大，是因为这种水出现的概率太大。
         
         
         4.CONSINE:
         (1)公式: CONSINE(A,B) =  P(AUB)/math.sqrt(P(A) * P(B)) = |P(A|B) * P(B|A)| = math.sqrt(置信度(A->B) * 置信度(B->A)) 
         
         (2)数学解释: 由于加平方根，使其不受到商品总数影响，也是反映相关性，推到公式最后本质是置信度来回的乘积。缺点是只是衡量相关性强弱，无法比较负相关和不想关。
         
         (3)业务解释: 这个公式稍微复杂, 它业务上主要体现 购买商品的前后关系。
         
         (4)举例说明：某超市发现顾客先买牙刷再买牙膏的概率很大，这种就是牙刷和牙膏的置信度大。但发现先买牙膏，再买牙刷的概率不大，这种就可以用CONSINE表示，它的大小是衡量物品之间先后关系的置信度，也就是先买A,再买B的概率大同时满足先买B再买A的概率也大，这个值才大。
         
         5.KUL:
         跟上面CONSINE意思一样，只是数学公式有区别，用的是加法。加法与乘法的区别在于，如果先买A，再买B的概率很大，先买B再买A的概率不太大，这个结果也大，但如果是乘法，就会比较小。准确说，这个指标对A和B的先后购买关系不是太看重，更看重的是单纯的置信度。
         
         
         6.不平衡比(IR):
         (1)公式: IR(A,B) = |SUP(A) - SUP(B)| / SUP(A) + SUP(B) - SUP(A∩B)
         (2)数学解释: A与B的支持度绝对值差与他俩之和和并集的差值的比值。
         (3)业务解释: 这个简单说就是A和B的购买概率相似，这个结果值就比较小，说明越平衡。
         (4)实际意义: 商家要找到相似支持度的商品关系时，就用这个指标很好。比如想找俩种牛奶销量相似的商品，比较他们的置信度，提升度，KUL等。那么先用这个指标来筛选即可。
         
         小知识: 
         (5)负相关:SUP(AUB) << SUP(A) * SUP(B)  零事务越多，会造成负相关。也就是A与B的并集远远小于A*B
         PS:通常KUL和IR在一起衡量更好一些，可以比较俩者的平衡，在相关性和提高度一定的前提下。IR越接近0越好。KUL超过我们的阈值 . 我认为,这个也是商家重要衡量指标，避免负相关的商品组合是商家要做的首要目标!

5.聚类相关算法
 整体定义: 根据一定规则的方法,将数据区分成几个类别。方法如下:
 a. 基于划分方法: 用迭代重定位技术通过中心点到各个点距离的平均值与各个点到该中心点的差值平方和最小来不停的划分。
 
 b. 基于层次方法: 将数据分层次,方法如下:
                 凝聚法:自底向上的方法,开始将每个对象作为单独的一个组,然后逐次合并相近的对象或组,直到合并结束或者某条件中止。
                 分裂法:所有对象放一个簇中,不停的划分更小的簇,一直到每个对象都在单独的一个簇中或者满足某条件中止。
             
                 用途：基于距离或者密度的数据。
                 缺点: 一层层合并/分裂，不能被撤销。这样虽然速度快，到容易分错。
 
 c. 基于密度方法：之前的划分和层次都是基于距离划分,这样只能发现球状簇,如果是任意形状的簇时比较困难。
                 
                 主题思想: 领域的密度(对象或数据点的数目)超过某个阈值,就继续增长给定的簇,也就是说对给定的簇中每个数据点,在给定的半径的领域中必须
                          包含最少数目的点。
                 
                 应用:此方法可以用来过滤噪声或离群点，发现任意形状的簇。
                 
 d. 基于网格方法: 对象空间量化为有限个单元,形成网格再处理。
                 优点: 处理速度快，时间复杂度跟量化后的单元数有关，跟对象个数无关。
                 缺点：过于依赖网格大小。
 
(1) Kmeans： E = Σk,i=1 ΣP∈Ci dist(P,Ci)^2
             P是对象,Ci是簇心。所以E是所有对象的误差平方和。最小化E
             不优化前，时间复杂度：O(nkt)  n是总数,k是维度,t是迭代次数
             优化：贪心.
             输入:  D, k个簇。
             超参: 初始化方法(随机/Kmeans++)
             输出: k个簇的集合
             过程: 
             (1) 普通方法:data为数据集,k为簇的个数
                 （1） 选择k个初始中心点，例如c[0]=data[0],…c[k-1]=data[k-1];
                 （2） 对于data[0]….data[n], 分别与c[0]…c[k-1]比较,这里是每个点与k个簇距离比较,就是n.k的时间复杂度,然后再排序找最小的,就是k.logk的时间复杂度。
                       最小的点就归为哪个簇。所以是n^k+1.logn的时间复杂度。
                 （3） 将该类所有坐标轴的均值成为一个新的点（这个点可能不存在）更新为新的簇心。(假设是俩个维度,就将该类所有横坐标的和除以个数列为横坐标，
                        纵坐标的和除以个数为纵坐标，成为一个新的点就是新簇心。)此时的时间复杂度为n.d。所以迭代一次的时间复杂度为O(n^dk+1.logn)总时间复杂度为:
                        j为迭代次数,O(j.(n^dk+1.logn))
                 （4） 重复(2)(3),直到所有c[i]值的变化小于给定阈值或者簇心位置不改变或者迭代次数完毕。
                 
             
             (2) 随机初始的方法:
                   从D中选择k个对象作为初始簇中心。
                   for i in 迭代次数:
                        将每个剩余的对象分配到最近的初始簇中;
                        随机选择一个非簇心的对象Orandom;
                        计算用Orandom代替代表对象Oj的总代价S;
                        if S<0,then Orandom替换Oj,形成新的k个代表对象的集合;
                   不再发生变化/迭代次数完毕。
                   
              缺点: k值不好确定,不适合非凸数据集,即噪点和离群点比较多的(所以也有Kmeans去离群点方法)。
              变种:(1) K-众数,当数据为标称数据(类别数据)时用众数取代均值进行上述聚类。 可以用K-众数和K-均值综合处理数值型和标称型数据。
              
                   (2) K-中心(PAM),利用相似误差解决噪点和离群点问题。
                       
                       选用簇中位置最中心的对象，试图对n个对象给出k个划分；代表对象也被称为是中心点，其他对象则被称为非代表对象；
                       最初随机选择k个对象作为中心点，该算法反复地用非代表对象来代替代表对象，试图找出更好的中心点，
                       以改进聚类的质量；在每次迭代中，所有可能的对象对被分析，每个对中的一个对象是中心点，而另一个是非代表对象。
                       对可能的各种组合，估算聚类结果的质量；一个对象Oi可以被使最大平方-误差值减少的对象代替；
                       在一次迭代中产生的最佳对象集合成为下次迭代的中心点。
                       
                       对比kmeans：    k-means是每次选簇的均值作为新的中心，迭代直到簇中对象分布不再变化。其缺点是对于离群点是敏感的，
                       因为一个具有很大极端值的对象会扭曲数据分布。那么我们可以考虑新的簇中心不选择均值而是选择簇内的某个对象，
                       只要使总的代价降低就可以。kmedoids算法比kmenas对于噪声和孤立点更鲁棒，
                       因为它最小化相异点对的和（minimizes a sum of pairwise dissimilarities ）
                       而不是欧式距离的平方和（sum of squared Euclidean distances.）。
                       一个中心点（medoid）可以这么定义：簇中某点的平均差异性在这一簇中所有点中最小。
                       
                       过程:
                       1） 任意选择k个对象作为初始的簇中心点
                       2） Repeat
                       3） 指派每个剩余对象给离他最近的中心点所表示的簇
                       4） Repeat
                       5） 选择一个未被选择的中心点Oi
                       6） Repeat
                       7） 选择一个未被选择过的非中心点对象Oh
                       8） 计算用Oh代替Oi的总代价并记录在S中
                       9） Until 所有非中心点都被选择过
                       10） Until 所有的中心点都被选择过
                       11） If 在S中的所有非中心点代替所有中心点后的计算出总代价有小于0的存在，
                            then找出S中的用非中心点替代中心点后代价最小的一个，并用该非中心点替代对应的中心点，
                            形成一个新的k个中心点的集合；
                       12） Until 没有再发生簇的重新分配，即所有的S都大于0.
                   
                   
              三者基于划分方法聚类的用途:
              (1) Kmeans: 针对数值型数据质量还可以的数据聚类。  时间复杂度: O(nkt)
              (2) K-众数: 针对标称数据聚类,基于频率的。         时间复杂度: O(nkt)
              (3) K-中心: 针对噪点和离群点比较大的数据,但不错的数据Kmeans更好，因为它用平方误差和取代均值当作新的中心点   时间复杂度: O(n(n-k)t)

              
              质点初始化方法：
              random	从输入数据表中随机采样出K个初始中心点，初始随机种子可以有参数seed指定
              topk	从输入表中读取前K行作为初始中心点
              uniform	从输入数据表，按最小到最大值，均匀计算出K个初始中心点
              kmpp	使用k-means++算法选出K个初始中心点，思路是第一个点随机选取,第二个点离第一个最远，第三个取前俩个和最远，依次类推...
              external	指定额外的初始中心表
              
              代码实现：
              1. Kmeans/K-众数
              #导入模块
              import numpy as np
              import matplotlib.pyplot as plt
              from math import sqrt
              
              #计算欧式距离
              def eucDistance(vec1,vec2):
                  return sqrt(sum(pow(vec2-vec1,2)))
              
              #初始聚类中心选择
              def initCentroids(dataSet,k):
                  numSamples,dim = dataSet.shape
                  centroids = np.zeros((k,dim))
                  for i in range(k):
                      index = int(np.random.uniform(0,numSamples))
                      centroids[i,:] = dataSet[index,:]
                  return centroids
              
              #K-means聚类算法，迭代
              def kmeanss(dataSet,k):
                  numSamples = dataSet.shape[0]
                  clusterAssement = np.mat(np.zeros((numSamples,2)))
                  clusterChanged = True
                  #  初始化聚类中心
                  centroids = initCentroids(dataSet,k)
                  while clusterChanged:
                      clusterChanged = False
                      for i in range(numSamples):
                          minDist = 100000.0
                          minIndex = 0
                          # 找到哪个与哪个中心最近
                          for j in range(k):
                              distance = eucDistance(centroids[j,:],dataSet[i,:])
                              if distance<minDist:
                                  minDist = distance
                                  minIndex = j
                            # 更新簇
                          clusterAssement[i,:] = minIndex,minDist**2
                          if clusterAssement[i,0]!=minIndex:
                              clusterChanged = True
                       # 坐标均值更新簇中心
                      for j in range(k):
                          pointsInCluster = dataSet[np.nonzero(clusterAssement[:0].A==j)[0]]
                          centroids[j,:] = np.mean(pointsInCluster,axis=0)
                  print('Congratulations,cluster complete!')
                  return centroids,clusterAssement
              
              #聚类结果显示
              def showCluster(dataSet,k,centroids,clusterAssement):
                  numSamples,dim = dataSet.shape
                  mark = ['or','ob','og','ok','^r','+r','<r','pr']
                  if k>len(mark):
                      print('Sorry!')
                      return 1
                  for i in np.xrange(numSamples):
                      markIndex = int(clusterAssement[i,0])
                      plt.plot(centroids[i,0],centroids[i,1],mark[i],markersize=12)
                  plt.show()

              
              
              2. K-中心
              import numpy as np
              import pandas as pd
              import copy
              
              df = np.loadtxt('waveform.txt',delimiter=',')    # 载入waveform数据集，22列，最后一列为标签0，1，2
              s= np.array(df)
              print(s.shape)
              print(s[0:10])
              
              data0 = s[s[:,s.shape[1]-1]==0][:100]   # 取标签为0的前100个样本
              data1 = s[s[:,s.shape[1]-1]==1][:100]   # 取标签为1的前100个样本
              data2 = s[s[:,s.shape[1]-1]==2][:100]   # 取标签为2的前100个样本
              
              data = np.array([data0,data1,data2])   
              data = data.reshape(-1,22)    
              
              def dis(data_a, data_b):
                  return np.sqrt(np.sum(np.square(data_a - data_b), axis=1))    # 返回欧氏距离
              
              def kmeans_wave(n=10, k=3, data=data):
                  data_new = copy.deepcopy(data)  # 前21列存放数据，不可变。最后1列即第22列存放标签，标签列随着每次迭代而更新。
                  data_now = copy.deepcopy(data)  # data_now用于存放中间过程的数据
              
                  center_point = np.random.choice(300,3,replace=False)
                  center = data_new[center_point,:20]   # 随机形成的3个中心，维度为（3，21）
              
              
                  distance = [[] for i in range(k)]
                  distance_now = [[] for i in range(k)]  # distance_now用于存放中间过程的距离
                  lost = np.ones([300,k])*float('inf')   # 初始lost为维度为（300，3）的无穷大
              
                  for j in range(k):   # 首先完成第一次划分，即第一次根据距离划分所有点到三个类别中
                      distance[j] = np.sqrt(np.sum(np.square(data_new[:,:20] - np.array(center[j])), axis=1))
                  data_new[:, 21] = np.argmin(np.array(distance), axis=0)  # data_new 的最后一列，即标签列随之改变，变为距离某中心点最近的标签，例如与第0个中心点最近，则为0
               
                  for i in range(n):    # 假设迭代n次
              
                      for m in range(k):   # 每一次都要分别替换k=3个中心点，所以循环k次。这层循环结束即算出利用所有点分别替代3个中心点后产生的900个lost值
              
                          for l in range(300):  # 替换某个中心点时都要利用全部点进行替换，所以循环300次。这层循环结束即算出利用所有点分别替换1个中心点后产生的300个lost值
              
                              center_now = copy.deepcopy(center)   # center_now用于存放中间过程的中心点
                              center_now[m] = data_now[l,:20]   # 用第l个点替换第m个中心点
                              for j in range(k):  # 计算暂时替换1个中心点后的距离值
                                  distance_now[j] = np.sqrt(np.sum(np.square(data_now[:,:20] - np.array(center_now[j])), axis=1))
                              data_now[:, 21] = np.argmin(np.array(distance), axis=0)  # data_now的标签列更新，注意data_now时中间过程，所以这里不能选择更新data_new的标签列
              
                              lost[l, m] = (dis(data_now[:, :20], center_now[data_now[:, 21].astype(int)]) \
                                     - dis(data_now[:, :20], center[data_new[:, 21].astype(int)])).sum()   # 这里很好理解lost的维度为什么为300*3了。lost[l,m]的值代表用第l个点替换第m个中心点的损失值
              
                      if np.min(lost) < 0:   # lost意味替换代价，选择代价最小的来完成替换
                          index = np.where(np.min(lost) == lost)  # 即找到min(lost)对应的替换组合
                          index_l = index[0][0]   # index_l指将要替代某个中心点的候选点
                          index_m = index[1][0]   # index_m指将要被替代的某个中心点，即用index_l来替代index_m
              
                      center[index_m] = data_now[index_l,:20]       #更新聚类中心
              
                      for j in range(k):
                          distance[j] = np.sqrt(np.sum(np.square(data_now[:, :20] - np.array(center[j])), axis=1))
                      data_new[:, 21] = np.argmin(np.array(distance), axis=0)  # 更新参考矩阵,至此data_new的标签列得以更新，即完成了一次迭代
              
                  return data_new  # 最后返回data_new，其最后一列即为最终聚好的标签
              
              
              if __name__ == '__main__':
                  data_new = kmeans_wave(10,3,data)
                  print(data_new.shape)
                  print(np.mean(data[:,21] == data_new[:,21]))   # 验证划分准确度
                        
    

(2) DBSCAN(密度)


(3) 层次
    a.凝聚分类层次: 自底向上的方法,开始将每个对象作为单独的一个组,然后逐次合并相近的对象或组,直到合并结束或者某条件中止。
                   单链接: 用欧式距离判断不同对象是否距离最短,然后合并成新的簇。比如a,b，然后下一层ab与cd最短，则分为abcd.也可以用第二层开始用
                   相似度来进行划分。如果是奇数,则为abc。以此类推。
                   
    b.分裂分类层次: 所有对象放一个簇中,不停的划分更小的簇,一直到每个对象都在单独的一个簇中或者满足某条件中止。 时间复杂度最大为2^n-1,
                   可以用启发式,即对已经分裂的地方不做回溯,降低时间复杂度,但这样会不准确，所以比较少用。
                   
    
    
    优点: 抗噪点和离群点能力强，但时间复杂度很高。
                   
    
    
    
    简单的代码实现:
    import numpy as np
    import data_helper
    np.random.seed(1)
    def get_raw_data(n):
     _data=np.random.rand(n,2)
     #生成数据的格式是n个（x,y）
     _groups={idx:[[x,y]] for idx,(x,y) in enumerate(_data)}
     return _groups
    def cal_distance(cluster1,cluster2):
     #采用最小距离作为聚类标准
     _min_distance=10000
     for x1,y1 in cluster1:
      for x2,y2 in cluster2:
       _distance=(x1-x2)**2+(y1-y2)**2
       if _distance<_min_distance:
        _min_distance=_distance
     return _distance
    groups=get_raw_data(10)
    count=0
    while len(groups)!=1:#判断是不是所有的数据是不是归为了同一类
     min_distance=10000
     len_groups=len(groups)
      
     for i in groups.keys():
      for j in groups.keys():
       if i>=j:
        continue
       distance=cal_distance(groups[i],groups[j])
       if distance<min_distance:
        min_distance=distance
        min_i=i
        min_j=j#这里的j>i
     groups[min_i].extend(groups.pop(min_j))
     data_helper.draw_data(groups)
     #一共n个簇，共迭代n-1次


(4) 高斯


6.深度学习RNN系列相关算法
(1)RNN
(2)LSTM
(3)优化。


7.传统时间序列相关算法



8.bagging类与boosting类以及延申的关于决策树算法总结




9.线性机器学习相关算法总结












二.框架与大数据相关技术
1.Spark
(1)调优问题
a.调优准则:
  (1) 同一个数据源尽量创建一个RDD,后续不同的业务逻辑可复用该RDD。
      原因:从数据源创建RDD涉及数据读取，读取数据要慢于数据读取。
  
  (2) 某个RDD进行多次不同transformation和action操作时,要将其在中间过程持久化,避免action重复操作触发作业时多次重复计算该RDD。
      每次action会触发作业从源头的RDD计算一遍来获得该RDD,所以中间步骤持久化到磁盘/内存避免从头计算。防止持久化时导致的数据丢失,
      可以设置CheckPoint进行中间保存。
  
  (3) 从数据源读取数据获得RDD后,尽早进行filter过滤掉不需要的数据。
      注意: filter后会获得大量小文件,增加读取次数,所以repartition/coalesce减少并行度。
      PS:并行度,指的是RDD分区数,一个分区对应一个Task。也可以是一个stage的Task数。默认每个task占用一个CPU Core处理。
         分区具有遗传性,新产生的RDD的分区数由parent RDD中最大分区数决定。 可以用rdd.partitions.size来获取RDD分区数。
         spark1.6后用rdd.getNumPartions()获取RDD的分区数。
         并行不够大时会导致资源闲置与浪费欸，比如一个程序分配了1000个Core,但一个stage只有30个Task。此时提高并行度可以提升硬件利用率。
         但如果并行度过大时，导致Task的创建与销毁过多从而影响性能。
  
  (4) 尽量避免使用shuffle算子,必须在shuffle时减少shuffle数据量。
      原因：shuffle时数据洗牌,很消耗性能。
      因为分布在集群中多个节点上包含同一个key的数据,拉取到同一个节点上时,并聚合或join操作,reduceByKey,join算子都会触发shuffle操作。
      各个节点相同的key会先写入本地磁盘中,然后其他节点需要通过网络传输拉取各个节点上的磁盘文件中的含有相同key的记录进行聚合,如果key过多，导致内存不足
      溢写到磁盘上，所以shuffle产生大量磁盘IO读写操作以及网络传输操作。避免reduceByKey,join,distinct,repartition这些shuffle操作。
      用Map类的非Shuffle算子比较好。如果必须用，可以先用map-side预聚合的算子减少Shuffle数据量。
      PS:map-side预聚合: 指的是每个节点本地map时对相同key记录进行聚合操作。类似MR的combiner。这样只有一个key记录，减少了磁盘IO和网络传输。
      
  (5) 使用高性能算子
      reduceByKey/aggregateByKey 取代 groupByKey.
      mapPartions替代普通的Map,因为mapPartitions类算子处理一个partition所有数据而不是一条，但容易OOM.
      当RDD执行filter后,如果过滤掉RDD中比较多的数据，就用coalescre算子减少RDD的分区数量,减少并行度，避免过多的Task开销。因为filter后
      数据量减少，开辟过多的Task速度反而会慢,因为用过多的Task的创建与销毁也是需要性能开销的。
      四种方法控制并行度的分区:
      a. 读取textFile类算子时,用minPartitions指定最小的分区数。
      b. 针对已存在的RDD用repartition()或coalesce()改变并行度。
      c. 针对大的RDD操作时自定义分区数而不是系统默认。
      d. spark.default.parallelism配置参数来调整并行度。并行度设置为集群总CPU Cores个数的2-3倍。比如Core为400.设置1000个Task,每个分区128MB是比较好的。
      
  
  (6) 大变量用广播   
      广播可以共享样本
  
  (7) Kryo优化序列化性能。
  (8) 用数据结构和算法来优化。
      考虑到Spark的底层时scala,scala的底层时java，这里介绍一些java的数据结构优化。
      a. 尽量减少对象来初始化数据。Java的对象都有对象头,对象头占用额外的16个字节(包含指向对象的指针等元数据信息),如果对象中一个int类型的变量,此时会占据
         所以假设一个int对象，就是16+4=20的内u才能空间,JSON取代。 一般用字符串代替对象。
      b. list,map有entry也消耗内存。 如果时List<Integer> list = new ArrayList<Integer> 用 int[] array = new int[]代替。
  (9) 资源调优:
      每台host服务器有N个Worker处理数据，一个master管理。
      一个Worker并行M个Executor进程。
      一个Executor占用多个core.
      Task分配到不同Executor执行且默认一个Task占用一个Core。
      多个task组合一个stage进行聚合。
      观察CPU使用率了解计算资源的使用情况,若CPU利用率很低且程序运行缓慢,尝试减少Executor占用的cores数量,增加Executor数量。
     
      a. spark.cores.max 指定core的数量,core越多越能快速分配task,但考虑到task的创建与销毁。每个Executor设置5个或以下的core
      b. spark.default.parallelism配置参数来调整并行度。并行度设置为集群总CPU Cores个数的2-3倍。比如Core为400.设置1000个Task,每个分区128MB是比较好的。 
      c. 
  
2.Tensorflow


3.ODPS(Hive)


4.shell调度


5.mysql同步













