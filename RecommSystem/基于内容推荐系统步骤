一. 算法方向：基于用户对物品自身属性的喜好而推荐的思路，更强调的是商品之间相似度和用户对商品喜好程度的预测，如果是历史数据小，则更多用商品之间
相似度，而非用户历史的评分。

二. 算法步骤：
 1. 离线部分：
    (1) 数据处理，特征提取
    (2) 用户画像建模
 2. 在线预测:
    主要是基于离线的过滤(召回)，排序结果。
 3. 模型部署(PASS,属于工程部份)
 
三. 步骤详解：
 1. 特征提取：
   （1）商品：非结构化 --> 一般是文本描述(图像视频先不考虑)。文本描述提取后有俩个方式：映射到多维空间，或者分词后提取关键字用权重进行排序起到划分效果。
             结构化   --> 利用商品自身的已有属性：价格，品类，颜色这些重要特征去划分商品效果。
             商品画像 --> 利用商品自身属性再构建出新的标签，这种就是起到提前划分商品的作用。
    案例：a. 电影推荐中有电影描述非结构化和电影的导演，主演等结构化属性，抽取非结构化的关键字进行权重排序，再配合结构化进行比较时合理的，也可以用自动学习
    来加权。
         b. WEB推荐
         c. 音乐推荐: 与电影性质一样，非结构化，结构化，商品/用户画像的融合。
 
 2. 特征表示与清洗 
    (1) 非结构化处理
        a. 停止词：  百度搜索+自己业务内部过滤。 一般冠词和介词，连词，代词视为停止词。
        b. 词干提取: 合并同一个词不同变形，比如同一个单词的单复数或不同时态被合并。比如hoping和hope这样的词汇合并成共同的词根"hop".这里不要盲目提取,
           比如hop这样的词在不同语境下有不同的含义。
        c. 短语提取: 主要提取频繁出现或者TF-IDF权重高的词汇，常用单词idf方法打折权重：id_i = log(n/ni). ni为在多少个文章里出现过,n为出现总数。
           另外，根据业务情况，当出现一些频率很高的垃圾数据(比如外部接入离谱的数据信息)时，可以用阻尼函数降低权重，即直接开平方根或者log. 
           比如:f(n) = log(n)或者根号n,最后tf-idf = f(n).id_i = log(n).log(n/ni)
           判断是否为垃圾的方法是看他的时间波动性而定，如果时间太少(<2)，则利用业务知识。 
           
        做完a,b,c后，提取出的关键词转化为向量空间。此时要注意一点，提取的词总数不要太大，根据文章数量控制在50-300之间,具体根据数据情况而定。
        原因是过多会导致一些噪声单词导致过拟合。它和停用词可以理解为特征选择步骤，TF-IDF可以理解为特征加权步骤，这些是无监督加权方法。还可以用
        有监督方法 -- 特征信息量度量：
        (1) 基尼系数: 适用于二元评分,顺序评分,分部在少量区间中的评分值。Gini(w) = 1 - Σi=1,t, pi(w)^2 . 基尼系数越小,说明区分能力越大,特征越重要
            t为评分商品的总数,w为该单词,pi(w)为w在这些商品中出现的概率。当 Gini(w) = 1 - 1时，说明这个w单词在所有评分中都存在，也就是它最重要。
            比如一个烂电影中，都出现了"难看"这个单词且分数都很低，那么说明这个单词会导致评分低的最重要因素之一。
            
        (2) 熵: 与基尼系数相似,基尼系数更容易理解，熵更容易有数学基础。公式: Entropy(w) = - Σi-1,t,pi(w).log(pi(w))
        
        (3) X^2统计量: 这个类似于关联分析。举例： 商品总共1000个，用户历史购买的商品占比为0.1，w单词在这些已经被购买的商品描述中，占比0.2.如下：
                           单词出现在该描述中                  单词未出现在该描述中 
            用户购买商品    O1 = 1000*0.1*0.2 = 20            O2 = 1000*0.1*0.8 = 80
            用户未购买商品  O3 = 1000*0.9*0.2 = 180           O4 = 1000*0.9*0.8 = 720 
            上述为一个w权重不太高的情况，但如果时很高时，会出现类似这种数据： O1=60，O2=40 (O1+O2=100), O3=140,O4=760 此时就说明用户购买的商品
            中出现该单词的概率很高，该单词是强特征。当我们验证俩个特征相关性时，可以用公式X^2 = Σi=1,p(Oi-Ei)^2/Ei。这个其实是卡方检验，但在
            这里我认为计算特征之间相关性也是可以的。俩组O分别为： O1=20,60  O2=80,40  O3=180,140 O4=720,760 
            最后得到 X^2 = (20-60)^2/60 + (40-80)^2/60 + (180-140)^2/140 + (720-760)^2/760 = 111.1 结果越大,说明独立性越强，相关性越弱。
            也就是说购买的商品中如果出现XX单词，出现另一个yyy单词的概率不高。
         
        (4)  评分偏差归一化： 将所有文章中出现该单词的评分的平均分设置为 u+ ,没出现该单词的评分的平均分设置为 u- 该文章的方差为σ，
             Dev = |u+ -u-|/σ， Dev越大，说明区分度越大，说明w权重比较高。
        以上四个评分方式要根据业务来定量，也可在基础上混合使用或者修改公式使用。
      
     (2) 特征加权： 除了TF-IDF外，还要根据业务去衡量，或者上述四个特征提取公式来融合，比如电影中，电影的导演和主演要比电影描述关键字的权重要高。
                   在加权过程中，可以学习使用熵，基尼系数的倒数或者修改公式来酌情处理。启发式函数可以使用，举例：考虑单词w的加权函数g(w),其中
                   α是大于1的参数： g(w) = α - Gini(w)。该公式得到的权重g(w)始终位于(α-1,α)区间，改变α的值，可以控制加权过程的灵敏度α，α越小，
                   灵敏度越大，然后将w在向量中自身算出的权重 h 乘以g(w) 得到新的权重结果f(w) = h(w).g(w) α是我们要调节的超参，
                   往往根业务还有关系。
                   
     (3) 
     
       
        
            
            
