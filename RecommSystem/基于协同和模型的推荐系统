一.简介
    在有监督的场景中,回归,boosting,deep_learning等有监督方法都可以泛化成协同过滤的场景,其作用是协同过滤矩阵的补全。
有监督：m*n矩阵中,前n-1列为已知,第n列为部分已知,n为label,已经知道的为训练样本,未知的为预测样本。
协同过滤: m*n矩阵中,已知的和未知的都是随机出现。所以协同过滤的训练和测试界限不清,由于根据m,n量级导致评分机制不同
所以可从用户角度又可从商品角度俩种来做。
    有监督分类算法一般研究更成熟的领域,严格来说分类算法是协同过滤的一种,比如bagging,boosting,模型组合均可以扩展为
协同过滤。


二. 决策树与回归树
    决策树：分类。回归树：回归. 一般用基尼系数区分：G(S) = 1 - Σi=1,r,pi^2, G(S)位于[0,1]区间,数字越小区分度越大。
一次划分的整体基尼指数=划分得到的孩子结点的基尼指数的加权平均。权值被定义为孩子节点包含的数据量。如果S1,S2为S孩子节点，
则： Gini(S=>[S1,S2]) = (n1.G(S1)+n2.G(S2))/ (n1+n2)。每次选取基尼系数最小点划分,也可以提前剪枝处理。在预测一个未知量时，需要将其特征对应的
自变量与决策树中从根节点到叶节点的一条路径匹配。因为决策树是层次去划分数据空间，所以测试数据会恰好匹配一条从根节点到叶节点的路径。
此时叶节点对应的标签为测试数据的类别。
    当处理数值型，可以用方差取代基尼系数，因为低方差表示该数值训练时因变量的取值被区别对待了。
    
三. 决策树扩展到协同过滤 
    1.方法：建立m*n矩阵后,m为用户数量,n为物品数目,对于任意一个属性(物品),我们需要将其他的变量当作自变量,建立一棵独立的决策树,所以决策树的数目等于
n(物品)的数目即可。当预测某个用户对于给定物品的评分时,只需要使用与待预测物品对应的决策树即可。
    2.问题：无法得到评分的物品,决策树无法找到合理的阈值判断该物品的各个属性是高于这个阈值还是低于。
    3.解决方法： 降维法：预测第j个物品评分时，将j外的m*n-1转化为m*d,d<<n-1。且降低到的d能达到所有属性都是已知的。这样所有用户降低到没有缺失值的状态再用决策树
    做预测。这样,我们实际是用n个样本做为训练集,d个样本作为训练的label，n-d为测试的label,而feature为n个样本的用户属性+d个样本的物品属性
    
四. 关联规则与协同过滤
    前提：物品被购买为1，没有为0.
    1. 预先确定最小支持度和置信度作为最大化参数,且保证该商品组合被该用户A购买过,如果一条关联规则的前件表示物品包含于A喜好的物品集合,则称该规则
    是A能触发的，所有触发规则按照置信度排序，排序好后前k个后件即是被推荐给A的物品,这个方法前提是被推荐物品不能多，而且符合该条件的并不多，所以
    我们要用评分-物品组合当作伪物品处理，比如一个伪物品可以是(物品=面包,评分=不喜欢)。基于这些伪物品产生新的事务,进而，之前讨论的一元矩阵可以产生关联
    规则：如下：
    (物品 = 面包，评分 = 喜欢) => (物品 = 鸡蛋，评分 = 喜欢);
    (物品 = 面包,评分  喜欢) AND (物品 = 鱼，屏风  不喜欢) => (物品  鸡蛋,评分 = 不喜欢)
    所以出发规则是检查前件是否包含该用户的某些伪物品来确定的，这些规则按照置信度从高到低排序。排序后规则的后件进一步被用来确定前k个需要被推荐给用
    户的伪物品。此时要考虑处理伪物品冲突，比如(物品=面包,不喜欢) AND (物品=面白,不喜欢)。此时要进行去重或者聚集操作，进而重新排序。
    
    2. 延申：关联规则的协同过滤不仅仅只是针对商品的前后关系，根据不同的指标，顺序不同也可以(比如kul值来计算而非置信度)。
       另外，关联规则最重要的是可以用在用户与用户之间的关系，通过商品 => A用户对B用户喜欢的品类，或者商品的置信度/KUL值来推测出喜好程度以便打分。
       最重要的是，不仅仅是用户和商品，更重要可以用在用户画像上，XXX标签的用户对XXX用户的关联程度，XXX标签的商品与XXX标签的商品的关联程度。
       最终可以计算出 => XXX标签的用户对XXX标签的商品的喜好程度，以此无限延申下去。
       
 五. 朴素贝叶斯协同过滤
    大概思路是将已经能通过显示或隐式的用户对物品的评分当作 P(B) P(A)为要预测的评分 => P(A|B) = P(A).P(B|A)/P(B)
    案例见书籍：63P-64P
 
 六.处理过拟合
    比如从来没有人对j物品评分过,用拉普拉斯平滑处理：P(Ruj = Vs) = qs + α / (Σqt + lα)
    如果第j个物品没有得到评分,该公式对每个可能的评分值使用预设的先验概率 1/L, α值用来控制平滑的程度，α越大结果越平滑，但是原始数据越不敏感。
    只需令分子分母分别加α和l.α使用类似的方法估计 P（Ruk|Ruj=Vs）
    
七. 黑盒机制 
    (1) 每一行分别作为目标列,其他列作为特征列,使用算法A估计缺失值，剩余的列使用当前的数据集合来创建一个包含完整的特征值矩阵,然后用目标列的已知值
    作为训练数据来预测缺失值。
    (2) 基于算法A对目标列的预测结果更新所有缺失值。
    案例：65P-66P，因为这次不用神经网络，后续补上。
    
八. 潜在因子模型(ALS)略，这部分网上一大堆。具体看：67P-99P。这次不用潜在因子，先略。

    
    
    
    
    
    


